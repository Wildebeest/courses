{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'data/fish'\n",
    "trainpath = path + '/train'\n",
    "validpath = path + '/valid'\n",
    "models_path = path+'/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "if not os.path.exists(models_path):\n",
    "    os.makedirs(models_path)\n",
    "\n",
    "valid_perc = 0.15\n",
    "for fish_type in os.listdir(trainpath):\n",
    "    type_train_path = os.path.join(trainpath, fish_type)\n",
    "    type_valid_path = os.path.join(validpath, fish_type)\n",
    "    if not os.path.exists(type_valid_path):\n",
    "        os.makedirs(type_valid_path)\n",
    "        images = os.listdir(type_train_path)\n",
    "        valid_images = random.sample(images, int(len(images) * valid_perc))\n",
    "        for image in valid_images:\n",
    "            os.rename(os.path.join(type_train_path, image), os.path.join(type_valid_path, image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.core import Lambda, Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "import numpy as np\n",
    "\n",
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape((3, 1, 1))\n",
    "def vgg_preprocess(x):\n",
    "    # subtracts the mean so that we get a 0-centered value\n",
    "    x = x - vgg_mean\n",
    "    \n",
    "    # reverses the axis, since most pretrained data comes from OpenCV, which uses BGR rather than RGB\n",
    "    return x[:, ::-1]\n",
    "\n",
    "def vgg_convblock(model, layers, filters):\n",
    "    for _ in xrange(0, layers):\n",
    "        model.add(Convolution2D(filters, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(vgg_preprocess, input_shape=(3, 224, 224), output_shape=(3,224,224)))\n",
    "vgg_convblock(model, 2, 64)\n",
    "vgg_convblock(model, 2, 128)\n",
    "vgg_convblock(model, 3, 256)\n",
    "vgg_convblock(model, 3, 512)\n",
    "vgg_convblock(model, 3, 512)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "download_path = 'http://files.fast.ai/models/'\n",
    "weights_file = get_file('vgg16.h5', download_path+'vgg16.h5', cache_subdir='models')\n",
    "model.load_weights(weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_layers = [layer for layer in model.layers if type(layer) in [Convolution2D, MaxPooling2D]]\n",
    "for layer in conv_layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_5 (BatchNorma (None, 3, 224, 224)   12          batchnormalization_input_2[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_14 (Convolution2D) (None, 64, 224, 224)  1792        lambda_2[0][0]                   \n",
      "                                                                   batchnormalization_5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_15 (Convolution2D) (None, 64, 224, 224)  36928       convolution2d_14[0][0]           \n",
      "                                                                   convolution2d_14[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_6 (MaxPooling2D)    (None, 64, 112, 112)  0           convolution2d_15[0][0]           \n",
      "                                                                   convolution2d_15[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_16 (Convolution2D) (None, 128, 112, 112) 73856       maxpooling2d_6[0][0]             \n",
      "                                                                   maxpooling2d_6[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_17 (Convolution2D) (None, 128, 112, 112) 147584      convolution2d_16[0][0]           \n",
      "                                                                   convolution2d_16[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_7 (MaxPooling2D)    (None, 128, 56, 56)   0           convolution2d_17[0][0]           \n",
      "                                                                   convolution2d_17[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_18 (Convolution2D) (None, 256, 56, 56)   295168      maxpooling2d_7[0][0]             \n",
      "                                                                   maxpooling2d_7[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_19 (Convolution2D) (None, 256, 56, 56)   590080      convolution2d_18[0][0]           \n",
      "                                                                   convolution2d_18[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_20 (Convolution2D) (None, 256, 56, 56)   590080      convolution2d_19[0][0]           \n",
      "                                                                   convolution2d_19[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_8 (MaxPooling2D)    (None, 256, 28, 28)   0           convolution2d_20[0][0]           \n",
      "                                                                   convolution2d_20[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_21 (Convolution2D) (None, 512, 28, 28)   1180160     maxpooling2d_8[0][0]             \n",
      "                                                                   maxpooling2d_8[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_22 (Convolution2D) (None, 512, 28, 28)   2359808     convolution2d_21[0][0]           \n",
      "                                                                   convolution2d_21[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_23 (Convolution2D) (None, 512, 28, 28)   2359808     convolution2d_22[0][0]           \n",
      "                                                                   convolution2d_22[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_9 (MaxPooling2D)    (None, 512, 14, 14)   0           convolution2d_23[0][0]           \n",
      "                                                                   convolution2d_23[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_24 (Convolution2D) (None, 512, 14, 14)   2359808     maxpooling2d_9[0][0]             \n",
      "                                                                   maxpooling2d_9[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_25 (Convolution2D) (None, 512, 14, 14)   2359808     convolution2d_24[0][0]           \n",
      "                                                                   convolution2d_24[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_26 (Convolution2D) (None, 512, 14, 14)   2359808     convolution2d_25[0][0]           \n",
      "                                                                   convolution2d_25[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_10 (MaxPooling2D)   (None, 512, 7, 7)     0           convolution2d_26[0][0]           \n",
      "                                                                   convolution2d_26[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_6 (BatchNorma (None, 512, 7, 7)     2048        maxpooling2d_10[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 25088)         0           batchnormalization_6[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 4096)          102764544   flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_7 (BatchNorma (None, 4096)          16384       dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 4096)          0           batchnormalization_7[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 4096)          16781312    dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_8 (BatchNorma (None, 4096)          16384       dense_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 4096)          0           batchnormalization_8[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 8)             32776       dropout_8[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 134,328,148\n",
      "Trainable params: 119,596,046\n",
      "Non-trainable params: 14,732,102\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "model = Sequential([BatchNormalization(axis=1, input_shape=(3, 224, 224))] + conv_layers + [\n",
    "    BatchNormalization(axis=1),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "    Dense(4096, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(8, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def batch_gen(path, class_mode='categorical', shuffle=True):\n",
    "    return ImageDataGenerator().flow_from_directory(path, target_size=(224, 224), batch_size=32, class_mode=class_mode, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = batch_gen(trainpath)\n",
    "valid_gen = batch_gen(validpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 103s - loss: 0.4280 - acc: 0.8852 - val_loss: 0.2646 - val_acc: 0.9183\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 96s - loss: 0.2117 - acc: 0.9449 - val_loss: 0.1634 - val_acc: 0.9520\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 95s - loss: 0.1351 - acc: 0.9633 - val_loss: 0.1347 - val_acc: 0.9680\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 95s - loss: 0.0702 - acc: 0.9779 - val_loss: 0.1697 - val_acc: 0.9627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xacc7bb70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(models_path+'first_cut_dropout.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3214/3214 [==============================] - 101s - loss: 0.0751 - acc: 0.9804 - val_loss: 0.1114 - val_acc: 0.9734\n",
      "Epoch 2/3\n",
      "3214/3214 [==============================] - 95s - loss: 0.0483 - acc: 0.9872 - val_loss: 0.0946 - val_acc: 0.9769\n",
      "Epoch 3/3\n",
      "3214/3214 [==============================] - 97s - loss: 0.0537 - acc: 0.9872 - val_loss: 0.2305 - val_acc: 0.9645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xacc87f98>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=3, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(models_path+'second_cut_dropout.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3214/3214 [==============================] - 116s - loss: 0.2514 - acc: 0.9571 - val_loss: 0.3904 - val_acc: 0.9520\n",
      "Epoch 2/3\n",
      "3214/3214 [==============================] - 108s - loss: 0.2511 - acc: 0.9586 - val_loss: 0.3589 - val_acc: 0.9414\n",
      "Epoch 3/3\n",
      "3214/3214 [==============================] - 108s - loss: 0.1957 - acc: 0.9689 - val_loss: 0.7567 - val_acc: 0.9076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x9e548cc0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.00001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=3, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(models_path+'second_cut.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a test submission for part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "testpath = path + '/test_stg1'\n",
    "test_gen = batch_gen(testpath, class_mode=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_one = model.predict_generator(test_gen, test_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_00005.jpg</td>\n",
       "      <td>1.379791e-11</td>\n",
       "      <td>5.973196e-07</td>\n",
       "      <td>3.163154e-07</td>\n",
       "      <td>6.828305e-06</td>\n",
       "      <td>9.999923e-01</td>\n",
       "      <td>4.275206e-09</td>\n",
       "      <td>6.931159e-08</td>\n",
       "      <td>8.481951e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_00007.jpg</td>\n",
       "      <td>9.999681e-01</td>\n",
       "      <td>4.622759e-06</td>\n",
       "      <td>7.769816e-06</td>\n",
       "      <td>1.071996e-05</td>\n",
       "      <td>9.783965e-08</td>\n",
       "      <td>7.005393e-06</td>\n",
       "      <td>1.695339e-06</td>\n",
       "      <td>5.674136e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_00009.jpg</td>\n",
       "      <td>9.993801e-01</td>\n",
       "      <td>4.911573e-04</td>\n",
       "      <td>6.267855e-05</td>\n",
       "      <td>5.240328e-06</td>\n",
       "      <td>4.571746e-06</td>\n",
       "      <td>4.364665e-05</td>\n",
       "      <td>1.058294e-05</td>\n",
       "      <td>1.979297e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_00018.jpg</td>\n",
       "      <td>9.999210e-01</td>\n",
       "      <td>3.250346e-06</td>\n",
       "      <td>6.027666e-06</td>\n",
       "      <td>4.965854e-07</td>\n",
       "      <td>1.026195e-07</td>\n",
       "      <td>3.665816e-05</td>\n",
       "      <td>2.063032e-05</td>\n",
       "      <td>1.176218e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_00027.jpg</td>\n",
       "      <td>4.664029e-02</td>\n",
       "      <td>5.491795e-01</td>\n",
       "      <td>1.300267e-03</td>\n",
       "      <td>1.486103e-03</td>\n",
       "      <td>1.563983e-03</td>\n",
       "      <td>2.944008e-01</td>\n",
       "      <td>1.618629e-02</td>\n",
       "      <td>8.924289e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image           ALB           BET           DOL           LAG  \\\n",
       "0  img_00005.jpg  1.379791e-11  5.973196e-07  3.163154e-07  6.828305e-06   \n",
       "1  img_00007.jpg  9.999681e-01  4.622759e-06  7.769816e-06  1.071996e-05   \n",
       "2  img_00009.jpg  9.993801e-01  4.911573e-04  6.267855e-05  5.240328e-06   \n",
       "3  img_00018.jpg  9.999210e-01  3.250346e-06  6.027666e-06  4.965854e-07   \n",
       "4  img_00027.jpg  4.664029e-02  5.491795e-01  1.300267e-03  1.486103e-03   \n",
       "\n",
       "            NoF         OTHER         SHARK           YFT  \n",
       "0  9.999923e-01  4.275206e-09  6.931159e-08  8.481951e-09  \n",
       "1  9.783965e-08  7.005393e-06  1.695339e-06  5.674136e-08  \n",
       "2  4.571746e-06  4.364665e-05  1.058294e-05  1.979297e-06  \n",
       "3  1.026195e-07  3.665816e-05  2.063032e-05  1.176218e-05  \n",
       "4  1.563983e-03  2.944008e-01  1.618629e-02  8.924289e-02  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "classes = sorted(train_gen.class_indices, key=train_gen.class_indices.get)\n",
    "submission_one = pd.DataFrame(predictions_one, columns=classes)\n",
    "submission_one.insert(0, 'image', [os.path.basename(filename) for filename in test_gen.filenames])\n",
    "submission_one.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# submission_one.to_csv(os.path.join(path, 'stage_one.csv.gz'), index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a test submission for part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "testpath = path + '/test_stg2'\n",
    "test_gen = batch_gen(testpath, class_mode=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_two = model.predict_generator(test_gen, test_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_stg2/image_00001.jpg</td>\n",
       "      <td>0.369125</td>\n",
       "      <td>2.196758e-02</td>\n",
       "      <td>5.095143e-04</td>\n",
       "      <td>0.039584</td>\n",
       "      <td>0.083023</td>\n",
       "      <td>4.181690e-01</td>\n",
       "      <td>1.285864e-04</td>\n",
       "      <td>0.067494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_stg2/image_00002.jpg</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>5.763921e-02</td>\n",
       "      <td>1.185645e-02</td>\n",
       "      <td>0.049786</td>\n",
       "      <td>0.815139</td>\n",
       "      <td>2.100842e-02</td>\n",
       "      <td>2.963617e-03</td>\n",
       "      <td>0.038815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_stg2/image_00003.jpg</td>\n",
       "      <td>0.999793</td>\n",
       "      <td>8.113956e-07</td>\n",
       "      <td>9.289097e-07</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>1.278712e-07</td>\n",
       "      <td>4.081627e-07</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_stg2/image_00004.jpg</td>\n",
       "      <td>0.010733</td>\n",
       "      <td>6.074815e-03</td>\n",
       "      <td>8.940433e-03</td>\n",
       "      <td>0.051236</td>\n",
       "      <td>0.917342</td>\n",
       "      <td>5.015638e-03</td>\n",
       "      <td>1.849269e-04</td>\n",
       "      <td>0.000473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_stg2/image_00005.jpg</td>\n",
       "      <td>0.996308</td>\n",
       "      <td>1.009451e-04</td>\n",
       "      <td>1.893483e-04</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>1.078976e-05</td>\n",
       "      <td>9.178784e-06</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image       ALB           BET           DOL       LAG  \\\n",
       "0  test_stg2/image_00001.jpg  0.369125  2.196758e-02  5.095143e-04  0.039584   \n",
       "1  test_stg2/image_00002.jpg  0.002792  5.763921e-02  1.185645e-02  0.049786   \n",
       "2  test_stg2/image_00003.jpg  0.999793  8.113956e-07  9.289097e-07  0.000045   \n",
       "3  test_stg2/image_00004.jpg  0.010733  6.074815e-03  8.940433e-03  0.051236   \n",
       "4  test_stg2/image_00005.jpg  0.996308  1.009451e-04  1.893483e-04  0.000175   \n",
       "\n",
       "        NoF         OTHER         SHARK       YFT  \n",
       "0  0.083023  4.181690e-01  1.285864e-04  0.067494  \n",
       "1  0.815139  2.100842e-02  2.963617e-03  0.038815  \n",
       "2  0.000141  1.278712e-07  4.081627e-07  0.000018  \n",
       "3  0.917342  5.015638e-03  1.849269e-04  0.000473  \n",
       "4  0.003056  1.078976e-05  9.178784e-06  0.000150  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "classes = sorted(train_gen.class_indices, key=train_gen.class_indices.get)\n",
    "submission_two = pd.DataFrame(predictions_two, columns=classes)\n",
    "submission_two.insert(0, 'image', ['test_stg2/' + os.path.basename(filename) for filename in test_gen.filenames])\n",
    "submission_two.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_00005.jpg</td>\n",
       "      <td>1.379791e-11</td>\n",
       "      <td>5.973196e-07</td>\n",
       "      <td>3.163154e-07</td>\n",
       "      <td>6.828305e-06</td>\n",
       "      <td>9.999923e-01</td>\n",
       "      <td>4.275206e-09</td>\n",
       "      <td>6.931159e-08</td>\n",
       "      <td>8.481951e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_00007.jpg</td>\n",
       "      <td>9.999681e-01</td>\n",
       "      <td>4.622759e-06</td>\n",
       "      <td>7.769816e-06</td>\n",
       "      <td>1.071996e-05</td>\n",
       "      <td>9.783965e-08</td>\n",
       "      <td>7.005393e-06</td>\n",
       "      <td>1.695339e-06</td>\n",
       "      <td>5.674136e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_00009.jpg</td>\n",
       "      <td>9.993801e-01</td>\n",
       "      <td>4.911573e-04</td>\n",
       "      <td>6.267855e-05</td>\n",
       "      <td>5.240328e-06</td>\n",
       "      <td>4.571746e-06</td>\n",
       "      <td>4.364665e-05</td>\n",
       "      <td>1.058294e-05</td>\n",
       "      <td>1.979297e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_00018.jpg</td>\n",
       "      <td>9.999210e-01</td>\n",
       "      <td>3.250346e-06</td>\n",
       "      <td>6.027666e-06</td>\n",
       "      <td>4.965854e-07</td>\n",
       "      <td>1.026195e-07</td>\n",
       "      <td>3.665816e-05</td>\n",
       "      <td>2.063032e-05</td>\n",
       "      <td>1.176218e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_00027.jpg</td>\n",
       "      <td>4.664029e-02</td>\n",
       "      <td>5.491795e-01</td>\n",
       "      <td>1.300267e-03</td>\n",
       "      <td>1.486103e-03</td>\n",
       "      <td>1.563983e-03</td>\n",
       "      <td>2.944008e-01</td>\n",
       "      <td>1.618629e-02</td>\n",
       "      <td>8.924289e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image           ALB           BET           DOL           LAG  \\\n",
       "0  img_00005.jpg  1.379791e-11  5.973196e-07  3.163154e-07  6.828305e-06   \n",
       "1  img_00007.jpg  9.999681e-01  4.622759e-06  7.769816e-06  1.071996e-05   \n",
       "2  img_00009.jpg  9.993801e-01  4.911573e-04  6.267855e-05  5.240328e-06   \n",
       "3  img_00018.jpg  9.999210e-01  3.250346e-06  6.027666e-06  4.965854e-07   \n",
       "4  img_00027.jpg  4.664029e-02  5.491795e-01  1.300267e-03  1.486103e-03   \n",
       "\n",
       "            NoF         OTHER         SHARK           YFT  \n",
       "0  9.999923e-01  4.275206e-09  6.931159e-08  8.481951e-09  \n",
       "1  9.783965e-08  7.005393e-06  1.695339e-06  5.674136e-08  \n",
       "2  4.571746e-06  4.364665e-05  1.058294e-05  1.979297e-06  \n",
       "3  1.026195e-07  3.665816e-05  2.063032e-05  1.176218e-05  \n",
       "4  1.563983e-03  2.944008e-01  1.618629e-02  8.924289e-02  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = submission_one.append(submission_two, ignore_index=True)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(path, 'stage_two.csv.gz'), index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a 3.7 private score. ( at the bottom :( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the first pseudo-labeled set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_preds = submission_two.sample(frac=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BET\n",
      "YFT\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "OTHER\n",
      "OTHER\n",
      "NoF\n",
      "SHARK\n",
      "NoF\n",
      "BET\n",
      "OTHER\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "OTHER\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "LAG\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "BET\n",
      "OTHER\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "OTHER\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "OTHER\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "LAG\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "YFT\n",
      "OTHER\n",
      "ALB\n",
      "SHARK\n",
      "BET\n",
      "NoF\n",
      "OTHER\n",
      "OTHER\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "OTHER\n",
      "OTHER\n",
      "ALB\n",
      "OTHER\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "YFT\n",
      "YFT\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "YFT\n",
      "OTHER\n",
      "ALB\n",
      "NoF\n",
      "OTHER\n",
      "BET\n",
      "YFT\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "OTHER\n",
      "NoF\n",
      "OTHER\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "OTHER\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "OTHER\n",
      "OTHER\n",
      "ALB\n",
      "ALB\n",
      "DOL\n",
      "BET\n",
      "ALB\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "YFT\n",
      "BET\n",
      "OTHER\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "SHARK\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "OTHER\n",
      "ALB\n",
      "OTHER\n",
      "BET\n",
      "ALB\n",
      "BET\n",
      "BET\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "YFT\n",
      "YFT\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "YFT\n",
      "BET\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "YFT\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "BET\n",
      "YFT\n",
      "YFT\n",
      "OTHER\n",
      "OTHER\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "LAG\n",
      "YFT\n",
      "ALB\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "OTHER\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "OTHER\n",
      "BET\n",
      "OTHER\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "OTHER\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "YFT\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "OTHER\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "OTHER\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "OTHER\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "OTHER\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "BET\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "LAG\n",
      "NoF\n",
      "OTHER\n",
      "YFT\n",
      "SHARK\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "LAG\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "OTHER\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "OTHER\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "LAG\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "OTHER\n",
      "ALB\n",
      "BET\n",
      "BET\n",
      "YFT\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "OTHER\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "OTHER\n",
      "BET\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "OTHER\n",
      "BET\n",
      "BET\n",
      "OTHER\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "OTHER\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "OTHER\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "YFT\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "OTHER\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "BET\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "OTHER\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "LAG\n",
      "BET\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "OTHER\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "OTHER\n",
      "NoF\n",
      "BET\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "OTHER\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "LAG\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "OTHER\n",
      "BET\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "BET\n",
      "OTHER\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "OTHER\n",
      "OTHER\n",
      "ALB\n",
      "OTHER\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "OTHER\n",
      "ALB\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "OTHER\n",
      "LAG\n",
      "BET\n",
      "BET\n",
      "OTHER\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "YFT\n",
      "OTHER\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "OTHER\n",
      "OTHER\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "OTHER\n",
      "ALB\n",
      "DOL\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "OTHER\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "BET\n",
      "BET\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "OTHER\n",
      "NoF\n",
      "BET\n",
      "YFT\n",
      "LAG\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "OTHER\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "YFT\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "LAG\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "YFT\n",
      "NoF\n",
      "DOL\n",
      "OTHER\n",
      "NoF\n",
      "YFT\n",
      "YFT\n",
      "NoF\n",
      "OTHER\n",
      "BET\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "OTHER\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "OTHER\n",
      "YFT\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "OTHER\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "OTHER\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "YFT\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "LAG\n",
      "YFT\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "BET\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "LAG\n",
      "ALB\n",
      "BET\n",
      "YFT\n",
      "YFT\n",
      "OTHER\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "OTHER\n",
      "NoF\n",
      "ALB\n",
      "OTHER\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "OTHER\n",
      "ALB\n",
      "OTHER\n",
      "SHARK\n",
      "NoF\n",
      "ALB\n",
      "OTHER\n",
      "YFT\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "OTHER\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "YFT\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "LAG\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "YFT\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "LAG\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "OTHER\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "YFT\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "SHARK\n",
      "LAG\n",
      "OTHER\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "OTHER\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "YFT\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "YFT\n",
      "BET\n",
      "OTHER\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "OTHER\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "OTHER\n",
      "ALB\n",
      "BET\n",
      "OTHER\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "OTHER\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "YFT\n",
      "OTHER\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "SHARK\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "OTHER\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "OTHER\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "SHARK\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "OTHER\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "OTHER\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "OTHER\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "BET\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "OTHER\n",
      "NoF\n",
      "NoF\n",
      "OTHER\n",
      "ALB\n",
      "ALB\n",
      "OTHER\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "OTHER\n",
      "YFT\n",
      "YFT\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "BET\n",
      "YFT\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "YFT\n",
      "BET\n",
      "BET\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "OTHER\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "OTHER\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "OTHER\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "OTHER\n",
      "BET\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "SHARK\n",
      "BET\n",
      "YFT\n",
      "ALB\n",
      "OTHER\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "YFT\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "BET\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "OTHER\n",
      "NoF\n",
      "OTHER\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "OTHER\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "OTHER\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "YFT\n",
      "LAG\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "BET\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "YFT\n",
      "BET\n",
      "LAG\n",
      "YFT\n",
      "YFT\n",
      "YFT\n",
      "OTHER\n",
      "YFT\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "OTHER\n",
      "OTHER\n",
      "ALB\n",
      "OTHER\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "YFT\n",
      "ALB\n",
      "OTHER\n",
      "YFT\n",
      "OTHER\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "BET\n",
      "OTHER\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "OTHER\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "SHARK\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "OTHER\n",
      "LAG\n",
      "ALB\n",
      "YFT\n",
      "DOL\n",
      "YFT\n",
      "YFT\n",
      "ALB\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "OTHER\n",
      "YFT\n",
      "YFT\n",
      "OTHER\n",
      "OTHER\n",
      "BET\n",
      "ALB\n",
      "OTHER\n",
      "OTHER\n",
      "BET\n",
      "BET\n",
      "YFT\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "YFT\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "OTHER\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "YFT\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "OTHER\n",
      "BET\n",
      "YFT\n",
      "OTHER\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "BET\n",
      "LAG\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "YFT\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "YFT\n",
      "OTHER\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "OTHER\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "YFT\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "NoF\n",
      "OTHER\n",
      "ALB\n",
      "BET\n",
      "OTHER\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "OTHER\n",
      "NoF\n",
      "OTHER\n",
      "OTHER\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "YFT\n",
      "BET\n",
      "DOL\n",
      "YFT\n",
      "ALB\n",
      "YFT\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "OTHER\n",
      "OTHER\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "SHARK\n",
      "ALB\n",
      "OTHER\n",
      "ALB\n",
      "OTHER\n",
      "YFT\n",
      "BET\n",
      "BET\n",
      "OTHER\n",
      "ALB\n",
      "NoF\n",
      "BET\n",
      "SHARK\n",
      "NoF\n",
      "ALB\n",
      "YFT\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "NoF\n",
      "YFT\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "NoF\n",
      "NoF\n",
      "BET\n",
      "ALB\n",
      "BET\n",
      "BET\n",
      "BET\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "OTHER\n",
      "YFT\n",
      "NoF\n",
      "LAG\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "ALB\n",
      "ALB\n",
      "NoF\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "ALB\n",
      "BET\n",
      "ALB\n",
      "NoF\n",
      "OTHER\n",
      "BET\n",
      "NoF\n",
      "OTHER\n",
      "ALB\n",
      "YFT\n",
      "NoF\n",
      "NoF\n",
      "NoF\n",
      "YFT\n",
      "OTHER\n",
      "OTHER\n",
      "OTHER\n",
      "YFT\n"
     ]
    }
   ],
   "source": [
    "pseudotrainpath = path + '/pseudotrain'\n",
    "if not os.path.exists(pseudotrainpath):\n",
    "    os.makedirs(pseudotrainpath)\n",
    "\n",
    "for pred in sample_preds.itertuples(index=False):\n",
    "    print(sample_preds.columns[np.argmax(pred[1:]) + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, looks like it's only predicting 3 categories. Let's try some data augmentation first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_gen(path, class_mode='categorical', shuffle=True, rotation_range=0., zoom_range=0., channel_shift_range=0., horizontal_flip=False, width_shift_range=0., height_shift_range=0.):\n",
    "    return ImageDataGenerator(rotation_range=rotation_range, zoom_range=zoom_range, channel_shift_range=channel_shift_range, horizontal_flip=horizontal_flip, width_shift_range=width_shift_range, height_shift_range=height_shift_range).flow_from_directory(path, target_size=(224, 224), batch_size=48, class_mode=class_mode, shuffle=shuffle)\n",
    "\n",
    "def image_aug(path):\n",
    "    return batch_gen(path, zoom_range=0.15, channel_shift_range=0.15, horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(trainpath)\n",
    "#model.load_weights(models_path+'second_cut_dropout.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3214/3214 [==============================] - 107s - loss: 0.8957 - acc: 0.8118 - val_loss: 0.2615 - val_acc: 0.9556\n",
      "Epoch 2/3\n",
      "3214/3214 [==============================] - 97s - loss: 0.5084 - acc: 0.8696 - val_loss: 0.1718 - val_acc: 0.9716\n",
      "Epoch 3/3\n",
      "3214/3214 [==============================] - 98s - loss: 0.3805 - acc: 0.8995 - val_loss: 0.3617 - val_acc: 0.9361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xacf7e7b8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=3, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(models_path+'second_cut_data_aug.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3214/3214 [==============================] - 105s - loss: 1.9706 - acc: 0.4499 - val_loss: 1.0610 - val_acc: 0.7087\n",
      "Epoch 2/3\n",
      "3214/3214 [==============================] - 95s - loss: 0.9041 - acc: 0.7362 - val_loss: 0.5288 - val_acc: 0.8757\n",
      "Epoch 3/3\n",
      "3214/3214 [==============================] - 92s - loss: 0.6179 - acc: 0.8158 - val_loss: 0.3274 - val_acc: 0.9165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xcb9f8b70>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.00003\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=3, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(models_path+'less_dropout.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "samplepath = path + '/sample'\n",
    "\n",
    "sample_perc = 0.10\n",
    "for fish_type in os.listdir(trainpath):\n",
    "    type_train_path = os.path.join(trainpath, fish_type)\n",
    "    type_sample_path = os.path.join(samplepath, fish_type)\n",
    "    if not os.path.exists(type_sample_path):\n",
    "        os.makedirs(type_sample_path)\n",
    "        images = os.listdir(type_train_path)\n",
    "        sample_images = random.sample(images, int(len(images) * sample_perc))\n",
    "        for image in sample_images:\n",
    "            shutil.copyfile(os.path.join(type_train_path, image), os.path.join(type_sample_path, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_31 (BatchNorm (None, 3, 1280, 720)  12          batchnormalization_input_6[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_66 (Convolution2D) (None, 64, 1280, 720) 1792        batchnormalization_31[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_67 (Convolution2D) (None, 64, 1280, 720) 36928       convolution2d_66[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_26 (MaxPooling2D)   (None, 64, 640, 360)  0           convolution2d_67[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_32 (BatchNorm (None, 64, 640, 360)  256         maxpooling2d_26[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_68 (Convolution2D) (None, 128, 640, 360) 73856       batchnormalization_32[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_69 (Convolution2D) (None, 128, 640, 360) 147584      convolution2d_68[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_27 (MaxPooling2D)   (None, 128, 320, 180) 0           convolution2d_69[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_33 (BatchNorm (None, 128, 320, 180) 512         maxpooling2d_27[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_70 (Convolution2D) (None, 256, 320, 180) 295168      batchnormalization_33[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_71 (Convolution2D) (None, 256, 320, 180) 590080      convolution2d_70[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_72 (Convolution2D) (None, 256, 320, 180) 590080      convolution2d_71[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_28 (MaxPooling2D)   (None, 256, 160, 90)  0           convolution2d_72[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_34 (BatchNorm (None, 256, 160, 90)  1024        maxpooling2d_28[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_73 (Convolution2D) (None, 512, 160, 90)  1180160     batchnormalization_34[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_74 (Convolution2D) (None, 512, 160, 90)  2359808     convolution2d_73[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_75 (Convolution2D) (None, 512, 160, 90)  2359808     convolution2d_74[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_29 (MaxPooling2D)   (None, 512, 80, 45)   0           convolution2d_75[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_35 (BatchNorm (None, 512, 80, 45)   2048        maxpooling2d_29[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_76 (Convolution2D) (None, 512, 80, 45)   2359808     batchnormalization_35[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_77 (Convolution2D) (None, 512, 80, 45)   2359808     convolution2d_76[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_78 (Convolution2D) (None, 512, 80, 45)   2359808     convolution2d_77[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_30 (MaxPooling2D)   (None, 512, 40, 22)   0           convolution2d_78[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_36 (BatchNorm (None, 512, 40, 22)   2048        maxpooling2d_30[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 512, 40, 22)   0           batchnormalization_36[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 450560)        0           dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1024)          461374464   flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_37 (BatchNorm (None, 1024)          4096        dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1024)          1049600     batchnormalization_37[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_38 (BatchNorm (None, 1024)          4096        dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 8)             8200        batchnormalization_38[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 477,161,044\n",
      "Trainable params: 477,153,998\n",
      "Non-trainable params: 7,046\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.core import Lambda, Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "def convblock(model, layers, filters):\n",
    "    for _ in xrange(0, layers):\n",
    "        model.add(Convolution2D(filters, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "model = Sequential([BatchNormalization(axis=1, input_shape=(3, 1280, 720))])\n",
    "convblock(model, 2, 64)\n",
    "convblock(model, 2, 128)\n",
    "convblock(model, 3, 256)\n",
    "convblock(model, 3, 512)\n",
    "convblock(model, 3, 512)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def batch_gen(path, class_mode='categorical', shuffle=True, rotation_range=0., zoom_range=0., channel_shift_range=0., horizontal_flip=False, width_shift_range=0., height_shift_range=0.):\n",
    "    return ImageDataGenerator(rotation_range=rotation_range, zoom_range=zoom_range, channel_shift_range=channel_shift_range, horizontal_flip=horizontal_flip, width_shift_range=width_shift_range, height_shift_range=height_shift_range).flow_from_directory(path, target_size=(1280, 720), batch_size=1, class_mode=class_mode, shuffle=shuffle)\n",
    "\n",
    "def image_aug(path):\n",
    "    return batch_gen(path, zoom_range=0.15, channel_shift_range=0.15, horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "samplepath = path + '/sample'\n",
    "train_gen = image_aug(trainpath)\n",
    "valid_gen = batch_gen(validpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3214/3214 [==============================] - 2776s - loss: 1.6948 - acc: 0.4294 - val_loss: 14.5298 - val_acc: 0.0977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d519198>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=1, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 2775s - loss: 1.6180 - acc: 0.4549 - val_loss: 14.8043 - val_acc: 0.0799\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 2771s - loss: 1.6179 - acc: 0.4549 - val_loss: 14.9876 - val_acc: 0.0693\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 2772s - loss: 1.6178 - acc: 0.4549 - val_loss: 14.9632 - val_acc: 0.0710\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 2774s - loss: 1.6178 - acc: 0.4549 - val_loss: 15.0875 - val_acc: 0.0639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x98354470>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+ '/models/fat_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpler architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_5 (BatchNorma (None, 3, 640, 360)   12          batchnormalization_input_2[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 64, 640, 360)  1792        batchnormalization_5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 64, 320, 180)  0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_6 (BatchNorma (None, 64, 320, 180)  256         maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 64, 320, 180)  36928       batchnormalization_6[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 64, 160, 90)   0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_7 (BatchNorma (None, 64, 160, 90)   256         maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 64, 160, 90)   36928       batchnormalization_7[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_6 (MaxPooling2D)    (None, 64, 80, 45)    0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_8 (BatchNorma (None, 64, 80, 45)    256         maxpooling2d_6[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 230400)        0           batchnormalization_8[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 64)            14745664    flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 8)             520         dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 14,822,612\n",
      "Trainable params: 14,822,222\n",
      "Non-trainable params: 390\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "path = 'data/fish'\n",
    "trainpath = path + '/train'\n",
    "validpath = path + '/valid'\n",
    "models_path = path+'/models/'\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.core import Lambda, Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "def convblock(model, layers, filters):\n",
    "    for _ in xrange(0, layers):\n",
    "        model.add(Convolution2D(filters, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(axis=1, input_shape=(3, 640, 360)))\n",
    "convblock(model, 1, 64)\n",
    "convblock(model, 1, 64)\n",
    "convblock(model, 1, 64)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "    #Dense(4096, activation='relu'),\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def batch_gen(path, class_mode='categorical', shuffle=True, rotation_range=0., zoom_range=0., channel_shift_range=0., horizontal_flip=False, width_shift_range=0., height_shift_range=0.):\n",
    "    return ImageDataGenerator(rotation_range=rotation_range, zoom_range=zoom_range, channel_shift_range=channel_shift_range, horizontal_flip=horizontal_flip, width_shift_range=width_shift_range, height_shift_range=height_shift_range).flow_from_directory(path, target_size=(640, 360), batch_size=16, class_mode=class_mode, shuffle=shuffle)\n",
    "\n",
    "def image_aug(path):\n",
    "    return batch_gen(path, zoom_range=0.15, channel_shift_range=0.15, horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(trainpath)\n",
    "valid_gen = batch_gen(validpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 204s - loss: 1.4913 - acc: 0.5361 - val_loss: 1.4117 - val_acc: 0.5364\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 199s - loss: 1.0146 - acc: 0.6686 - val_loss: 0.9203 - val_acc: 0.6323\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 199s - loss: 0.8507 - acc: 0.7119 - val_loss: 0.7769 - val_acc: 0.7407\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 197s - loss: 0.6812 - acc: 0.7791 - val_loss: 0.5740 - val_acc: 0.8117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4aa132e8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 203s - loss: 0.5991 - acc: 0.8077 - val_loss: 0.6049 - val_acc: 0.8028\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 195s - loss: 0.5467 - acc: 0.8223 - val_loss: 0.4095 - val_acc: 0.8703\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 197s - loss: 0.4885 - acc: 0.8388 - val_loss: 0.4001 - val_acc: 0.8508\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 199s - loss: 0.4282 - acc: 0.8516 - val_loss: 0.4098 - val_acc: 0.8597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4bf38ba8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 204s - loss: 0.4151 - acc: 0.8684 - val_loss: 0.4388 - val_acc: 0.8845\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 196s - loss: 0.3871 - acc: 0.8787 - val_loss: 0.3223 - val_acc: 0.9201\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 196s - loss: 0.3542 - acc: 0.8799 - val_loss: 0.3897 - val_acc: 0.8899\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 198s - loss: 0.3003 - acc: 0.9042 - val_loss: 0.3080 - val_acc: 0.9094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4bf38c50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 203s - loss: 0.2935 - acc: 0.9057 - val_loss: 0.3086 - val_acc: 0.9130\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 196s - loss: 0.2835 - acc: 0.9060 - val_loss: 0.3348 - val_acc: 0.9023\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 195s - loss: 0.2667 - acc: 0.9070 - val_loss: 0.3532 - val_acc: 0.8970\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 196s - loss: 0.2486 - acc: 0.9241 - val_loss: 0.2686 - val_acc: 0.9254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4bf38b38>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/custom_conv.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 205s - loss: 0.2402 - acc: 0.9250 - val_loss: 0.2573 - val_acc: 0.9343\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 204s - loss: 0.2289 - acc: 0.9294 - val_loss: 0.2403 - val_acc: 0.9325\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 199s - loss: 0.2154 - acc: 0.9319 - val_loss: 0.2913 - val_acc: 0.9201\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 196s - loss: 0.2057 - acc: 0.9396 - val_loss: 0.2569 - val_acc: 0.9290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4bf3ec88>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.00001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/custom_conv2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_00005.jpg</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>2.153253e-09</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>1.248368e-07</td>\n",
       "      <td>7.339363e-09</td>\n",
       "      <td>2.623337e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_00007.jpg</td>\n",
       "      <td>0.975450</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.238818e-02</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>5.866907e-04</td>\n",
       "      <td>1.409972e-06</td>\n",
       "      <td>1.122610e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_00009.jpg</td>\n",
       "      <td>0.934920</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>3.743373e-03</td>\n",
       "      <td>0.011239</td>\n",
       "      <td>2.678013e-02</td>\n",
       "      <td>1.637661e-05</td>\n",
       "      <td>1.845643e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_00018.jpg</td>\n",
       "      <td>0.798905</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>9.136594e-03</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>4.838634e-02</td>\n",
       "      <td>1.631585e-04</td>\n",
       "      <td>1.431641e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_00027.jpg</td>\n",
       "      <td>0.099715</td>\n",
       "      <td>0.181564</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>7.242331e-03</td>\n",
       "      <td>0.094296</td>\n",
       "      <td>4.663059e-03</td>\n",
       "      <td>3.152917e-02</td>\n",
       "      <td>5.801634e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image       ALB       BET       DOL           LAG       NoF  \\\n",
       "0  img_00005.jpg  0.000006  0.000032  0.000110  2.153253e-09  0.999852   \n",
       "1  img_00007.jpg  0.975450  0.000025  0.000003  2.238818e-02  0.000423   \n",
       "2  img_00009.jpg  0.934920  0.004835  0.000010  3.743373e-03  0.011239   \n",
       "3  img_00018.jpg  0.798905  0.000083  0.000132  9.136594e-03  0.000030   \n",
       "4  img_00027.jpg  0.099715  0.181564  0.000827  7.242331e-03  0.094296   \n",
       "\n",
       "          OTHER         SHARK           YFT  \n",
       "0  1.248368e-07  7.339363e-09  2.623337e-09  \n",
       "1  5.866907e-04  1.409972e-06  1.122610e-03  \n",
       "2  2.678013e-02  1.637661e-05  1.845643e-02  \n",
       "3  4.838634e-02  1.631585e-04  1.431641e-01  \n",
       "4  4.663059e-03  3.152917e-02  5.801634e-01  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testpath = path + '/test_stg1'\n",
    "test_gen = batch_gen(testpath, class_mode=None, shuffle=False)\n",
    "predictions_one = model.predict_generator(test_gen, test_gen.nb_sample)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "classes = sorted(train_gen.class_indices, key=train_gen.class_indices.get)\n",
    "submission_one = pd.DataFrame(predictions_one, columns=classes)\n",
    "submission_one.insert(0, 'image', [os.path.basename(filename) for filename in test_gen.filenames])\n",
    "submission_one.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12153 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_stg2/image_00001.jpg</td>\n",
       "      <td>8.142726e-11</td>\n",
       "      <td>0.999314</td>\n",
       "      <td>7.991391e-10</td>\n",
       "      <td>1.635908e-05</td>\n",
       "      <td>5.868961e-10</td>\n",
       "      <td>4.200497e-04</td>\n",
       "      <td>1.426409e-04</td>\n",
       "      <td>1.065265e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_stg2/image_00002.jpg</td>\n",
       "      <td>7.929316e-02</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>6.366926e-05</td>\n",
       "      <td>1.360887e-07</td>\n",
       "      <td>9.205092e-01</td>\n",
       "      <td>6.528909e-08</td>\n",
       "      <td>5.159534e-08</td>\n",
       "      <td>1.692773e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_stg2/image_00003.jpg</td>\n",
       "      <td>1.259260e-03</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>3.848965e-07</td>\n",
       "      <td>1.281208e-08</td>\n",
       "      <td>9.987276e-01</td>\n",
       "      <td>6.673592e-07</td>\n",
       "      <td>1.603590e-08</td>\n",
       "      <td>5.866092e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_stg2/image_00004.jpg</td>\n",
       "      <td>2.659394e-01</td>\n",
       "      <td>0.038392</td>\n",
       "      <td>6.429963e-05</td>\n",
       "      <td>8.403962e-03</td>\n",
       "      <td>1.494113e-01</td>\n",
       "      <td>9.197830e-07</td>\n",
       "      <td>1.901453e-05</td>\n",
       "      <td>5.377690e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_stg2/image_00005.jpg</td>\n",
       "      <td>2.939177e-02</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>2.137676e-05</td>\n",
       "      <td>2.333543e-05</td>\n",
       "      <td>9.684479e-01</td>\n",
       "      <td>7.837817e-07</td>\n",
       "      <td>2.958562e-06</td>\n",
       "      <td>7.244359e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image           ALB       BET           DOL  \\\n",
       "0  test_stg2/image_00001.jpg  8.142726e-11  0.999314  7.991391e-10   \n",
       "1  test_stg2/image_00002.jpg  7.929316e-02  0.000132  6.366926e-05   \n",
       "2  test_stg2/image_00003.jpg  1.259260e-03  0.000012  3.848965e-07   \n",
       "3  test_stg2/image_00004.jpg  2.659394e-01  0.038392  6.429963e-05   \n",
       "4  test_stg2/image_00005.jpg  2.939177e-02  0.001387  2.137676e-05   \n",
       "\n",
       "            LAG           NoF         OTHER         SHARK           YFT  \n",
       "0  1.635908e-05  5.868961e-10  4.200497e-04  1.426409e-04  1.065265e-04  \n",
       "1  1.360887e-07  9.205092e-01  6.528909e-08  5.159534e-08  1.692773e-06  \n",
       "2  1.281208e-08  9.987276e-01  6.673592e-07  1.603590e-08  5.866092e-08  \n",
       "3  8.403962e-03  1.494113e-01  9.197830e-07  1.901453e-05  5.377690e-01  \n",
       "4  2.333543e-05  9.684479e-01  7.837817e-07  2.958562e-06  7.244359e-04  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testpath = path + '/test_stg2'\n",
    "test_gen = batch_gen(testpath, class_mode=None, shuffle=False)\n",
    "predictions_two = model.predict_generator(test_gen, test_gen.nb_sample)\n",
    "\n",
    "classes = sorted(train_gen.class_indices, key=train_gen.class_indices.get)\n",
    "submission_two = pd.DataFrame(predictions_two, columns=classes)\n",
    "submission_two.insert(0, 'image', ['test_stg2/' + os.path.basename(filename) for filename in test_gen.filenames])\n",
    "submission_two.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_00005.jpg</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>2.153253e-09</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>1.248368e-07</td>\n",
       "      <td>7.339363e-09</td>\n",
       "      <td>2.623337e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_00007.jpg</td>\n",
       "      <td>0.975450</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.238818e-02</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>5.866907e-04</td>\n",
       "      <td>1.409972e-06</td>\n",
       "      <td>1.122610e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_00009.jpg</td>\n",
       "      <td>0.934920</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>3.743373e-03</td>\n",
       "      <td>0.011239</td>\n",
       "      <td>2.678013e-02</td>\n",
       "      <td>1.637661e-05</td>\n",
       "      <td>1.845643e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_00018.jpg</td>\n",
       "      <td>0.798905</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>9.136594e-03</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>4.838634e-02</td>\n",
       "      <td>1.631585e-04</td>\n",
       "      <td>1.431641e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_00027.jpg</td>\n",
       "      <td>0.099715</td>\n",
       "      <td>0.181564</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>7.242331e-03</td>\n",
       "      <td>0.094296</td>\n",
       "      <td>4.663059e-03</td>\n",
       "      <td>3.152917e-02</td>\n",
       "      <td>5.801634e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image       ALB       BET       DOL           LAG       NoF  \\\n",
       "0  img_00005.jpg  0.000006  0.000032  0.000110  2.153253e-09  0.999852   \n",
       "1  img_00007.jpg  0.975450  0.000025  0.000003  2.238818e-02  0.000423   \n",
       "2  img_00009.jpg  0.934920  0.004835  0.000010  3.743373e-03  0.011239   \n",
       "3  img_00018.jpg  0.798905  0.000083  0.000132  9.136594e-03  0.000030   \n",
       "4  img_00027.jpg  0.099715  0.181564  0.000827  7.242331e-03  0.094296   \n",
       "\n",
       "          OTHER         SHARK           YFT  \n",
       "0  1.248368e-07  7.339363e-09  2.623337e-09  \n",
       "1  5.866907e-04  1.409972e-06  1.122610e-03  \n",
       "2  2.678013e-02  1.637661e-05  1.845643e-02  \n",
       "3  4.838634e-02  1.631585e-04  1.431641e-01  \n",
       "4  4.663059e-03  3.152917e-02  5.801634e-01  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = submission_one.append(submission_two, ignore_index=True)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(path, 'stage_two_custom.csv.gz'), index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better, but still not great. Let's try a slightly different architecture.\n",
    "\n",
    "## Custom 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1080 Ti (CNMeM is disabled, cuDNN 5110)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_1 (BatchNorma (None, 3, 1280, 720)  12          batchnormalization_input_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 1280, 720) 1792        batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 1280, 720) 36928       convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 640, 360)  0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 64, 640, 360)  256         maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 64, 640, 360)  36928       batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 64, 640, 360)  36928       convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 64, 320, 180)  0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 64, 320, 180)  256         maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 64, 320, 180)  36928       batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 64, 320, 180)  36928       convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 64, 160, 90)   0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 64, 160, 90)   256         maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 64, 160, 90)   36928       batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 64, 160, 90)   36928       convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 64, 80, 45)    0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_5 (BatchNorma (None, 64, 80, 45)    256         maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 230400)        0           batchnormalization_5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           29491328    flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 128)           16512       dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 8)             1032        dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 29,770,196\n",
      "Trainable params: 29,769,678\n",
      "Non-trainable params: 518\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "path = 'data/fish'\n",
    "trainpath = path + '/train'\n",
    "validpath = path + '/valid'\n",
    "models_path = path+'/models/'\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.core import Lambda, Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "def convblock(model, layers, filters):\n",
    "    for _ in xrange(0, layers):\n",
    "        model.add(Convolution2D(filters, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(axis=1, input_shape=(3, 1280, 720)))\n",
    "convblock(model, 2, 64)\n",
    "convblock(model, 2, 64)\n",
    "convblock(model, 2, 64)\n",
    "convblock(model, 2, 64)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def batch_gen(path, class_mode='categorical', shuffle=True, rotation_range=0., zoom_range=0., channel_shift_range=0., horizontal_flip=False, width_shift_range=0., height_shift_range=0.):\n",
    "    return ImageDataGenerator(rotation_range=rotation_range, zoom_range=zoom_range, channel_shift_range=channel_shift_range, horizontal_flip=horizontal_flip, width_shift_range=width_shift_range, height_shift_range=height_shift_range).flow_from_directory(path, target_size=(1280, 720), batch_size=4, class_mode=class_mode, shuffle=shuffle)\n",
    "\n",
    "def image_aug(path):\n",
    "    return batch_gen(path, zoom_range=0.10, channel_shift_range=0.10, horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(trainpath)\n",
    "valid_gen = batch_gen(validpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 900s - loss: 1.2481 - acc: 0.5840 - val_loss: 0.8406 - val_acc: 0.7105\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 897s - loss: 0.8450 - acc: 0.7110 - val_loss: 0.5504 - val_acc: 0.8153\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 900s - loss: 0.6439 - acc: 0.7729 - val_loss: 0.5937 - val_acc: 0.7904\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 898s - loss: 0.5197 - acc: 0.8298 - val_loss: 0.3731 - val_acc: 0.8544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3394d6a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 897s - loss: 0.4287 - acc: 0.8597 - val_loss: 0.3443 - val_acc: 0.8774\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 897s - loss: 0.3650 - acc: 0.8759 - val_loss: 0.4177 - val_acc: 0.8774\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 897s - loss: 0.3297 - acc: 0.8917 - val_loss: 0.3506 - val_acc: 0.9023\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 900s - loss: 0.3097 - acc: 0.8992 - val_loss: 0.3008 - val_acc: 0.9076\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 899s - loss: 0.2706 - acc: 0.9113 - val_loss: 0.3262 - val_acc: 0.9059\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 896s - loss: 0.2308 - acc: 0.9238 - val_loss: 0.2620 - val_acc: 0.9183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5d2b7208>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 899s - loss: 0.2167 - acc: 0.9309 - val_loss: 0.2284 - val_acc: 0.9325\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 898s - loss: 0.2034 - acc: 0.9337 - val_loss: 0.2131 - val_acc: 0.9343\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 897s - loss: 0.1876 - acc: 0.9356 - val_loss: 0.2833 - val_acc: 0.9290\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 897s - loss: 0.1807 - acc: 0.9421 - val_loss: 0.1812 - val_acc: 0.9556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5d2b7160>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path + '/models/custom_larger.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "testpath = path + '/test_stg1'\n",
    "test_gen = batch_gen(testpath, class_mode=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_one = model.predict_generator(test_gen, test_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_00005.jpg</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_00007.jpg</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010861</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_00009.jpg</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.017057</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_00018.jpg</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.014938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_00027.jpg</td>\n",
       "      <td>0.888394</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.109281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image       ALB   BET   DOL       LAG   NoF     OTHER  SHARK  \\\n",
       "0  img_00005.jpg  0.010000  0.01  0.01  0.010000  0.92  0.010000   0.01   \n",
       "1  img_00007.jpg  0.920000  0.01  0.01  0.010861  0.01  0.010000   0.01   \n",
       "2  img_00009.jpg  0.920000  0.01  0.01  0.010000  0.01  0.017057   0.01   \n",
       "3  img_00018.jpg  0.920000  0.01  0.01  0.010000  0.01  0.010000   0.01   \n",
       "4  img_00027.jpg  0.888394  0.01  0.01  0.010000  0.01  0.010000   0.01   \n",
       "\n",
       "        YFT  \n",
       "0  0.010000  \n",
       "1  0.010000  \n",
       "2  0.010000  \n",
       "3  0.014938  \n",
       "4  0.109281  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/8, mx)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "classes = sorted(train_gen.class_indices, key=train_gen.class_indices.get)\n",
    "submission_one = pd.DataFrame(do_clip(predictions_one, 0.92), columns=classes)\n",
    "submission_one.insert(0, 'image', [os.path.basename(filename) for filename in test_gen.filenames])\n",
    "submission_one.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "testpath = path + '/test_stg2'\n",
    "test_gen = batch_gen(testpath, class_mode=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_two = model.predict_generator(test_gen, test_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_stg2/image_00001.jpg</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.485560</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.496071</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.011096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_stg2/image_00002.jpg</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_stg2/image_00003.jpg</td>\n",
       "      <td>0.035360</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_stg2/image_00004.jpg</td>\n",
       "      <td>0.696230</td>\n",
       "      <td>0.046385</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.083992</td>\n",
       "      <td>0.046925</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.126347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_stg2/image_00005.jpg</td>\n",
       "      <td>0.226159</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.766131</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image       ALB       BET   DOL       LAG       NoF  \\\n",
       "0  test_stg2/image_00001.jpg  0.010000  0.485560  0.01  0.010000  0.496071   \n",
       "1  test_stg2/image_00002.jpg  0.010000  0.010000  0.01  0.010000  0.920000   \n",
       "2  test_stg2/image_00003.jpg  0.035360  0.010000  0.01  0.010000  0.920000   \n",
       "3  test_stg2/image_00004.jpg  0.696230  0.046385  0.01  0.083992  0.046925   \n",
       "4  test_stg2/image_00005.jpg  0.226159  0.010000  0.01  0.010000  0.766131   \n",
       "\n",
       "   OTHER  SHARK       YFT  \n",
       "0   0.01   0.01  0.011096  \n",
       "1   0.01   0.01  0.010000  \n",
       "2   0.01   0.01  0.010000  \n",
       "3   0.01   0.01  0.126347  \n",
       "4   0.01   0.01  0.010000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = sorted(train_gen.class_indices, key=train_gen.class_indices.get)\n",
    "submission_two = pd.DataFrame(do_clip(predictions_two, 0.92), columns=classes)\n",
    "submission_two.insert(0, 'image', ['test_stg2/' + os.path.basename(filename) for filename in test_gen.filenames])\n",
    "submission_two.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = submission_one.append(submission_two, ignore_index=True)\n",
    "submission.to_csv(os.path.join(path, 'stage_two_custom_larger_clipping.csv.gz'), index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1080 Ti (CNMeM is disabled, cuDNN 5110)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_1 (BatchNorma (None, 3, 1280, 720)  12          batchnormalization_input_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 1280, 720) 1792        batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 640, 360)  0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 64, 640, 360)  256         maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 128, 640, 360) 73856       batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 320, 180) 0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 128, 320, 180) 512         maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 256, 320, 180) 295168      batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 256, 160, 90)  0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 256, 160, 90)  1024        maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 160, 90)  295040      batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 128, 80, 45)   0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_5 (BatchNorma (None, 128, 80, 45)   512         maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 64, 80, 45)    73792       batchnormalization_5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 64, 40, 22)    0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_6 (BatchNorma (None, 64, 40, 22)    256         maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 56320)         0           batchnormalization_6[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1024)          57672704    flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_7 (BatchNorma (None, 1024)          4096        dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 128)           131200      batchnormalization_7[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_8 (BatchNorma (None, 128)           512         dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 8)             1032        batchnormalization_8[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 58,551,764\n",
      "Trainable params: 58,548,174\n",
      "Non-trainable params: 3,590\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "path = 'data/fish'\n",
    "trainpath = path + '/train'\n",
    "samplepath = path + '/sample'\n",
    "validpath = path + '/valid'\n",
    "models_path = path+'/models/'\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.core import Lambda, Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "def convblock(model, layers, filters):\n",
    "    for _ in xrange(0, layers):\n",
    "        model.add(Convolution2D(filters, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(axis=1, input_shape=(3, 1280, 720)))\n",
    "convblock(model, 1, 64)\n",
    "convblock(model, 1, 128)\n",
    "convblock(model, 1, 256)\n",
    "convblock(model, 1, 128)\n",
    "convblock(model, 1, 64)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def batch_gen(path, class_mode='categorical', shuffle=True, rotation_range=0., zoom_range=0., channel_shift_range=0., horizontal_flip=False, width_shift_range=0., height_shift_range=0., batch_size=4):\n",
    "    return ImageDataGenerator(rotation_range=rotation_range, zoom_range=zoom_range, channel_shift_range=channel_shift_range, horizontal_flip=horizontal_flip, width_shift_range=width_shift_range, height_shift_range=height_shift_range).flow_from_directory(path, target_size=(1280, 720), batch_size=batch_size, class_mode=class_mode, shuffle=shuffle)\n",
    "\n",
    "def image_aug(path):\n",
    "    return batch_gen(path, zoom_range=0.10, channel_shift_range=0.10, horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(trainpath)\n",
    "valid_gen = batch_gen(validpath, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3214/3214 [==============================] - 710s - loss: 2.0914 - acc: 0.3258 - val_loss: 1.6697 - val_acc: 0.4813\n",
      "Epoch 2/2\n",
      "3214/3214 [==============================] - 706s - loss: 1.7590 - acc: 0.4147 - val_loss: 1.4700 - val_acc: 0.5240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2eebfc88>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.optimizer.lr = 0.00001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=2, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 712s - loss: 1.6036 - acc: 0.4757 - val_loss: 1.3454 - val_acc: 0.6039\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 712s - loss: 1.5337 - acc: 0.5050 - val_loss: 1.2101 - val_acc: 0.6838\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 702s - loss: 1.4715 - acc: 0.5134 - val_loss: 1.1802 - val_acc: 0.6590\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 702s - loss: 1.3839 - acc: 0.5600 - val_loss: 1.2984 - val_acc: 0.6803\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 704s - loss: 1.3426 - acc: 0.5790 - val_loss: 1.1486 - val_acc: 0.6163\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 700s - loss: 1.3152 - acc: 0.5831 - val_loss: 1.0705 - val_acc: 0.6803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x48e537f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(modelpath+'/custom3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 700s - loss: 1.2409 - acc: 0.6017 - val_loss: 0.9413 - val_acc: 0.6963\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 698s - loss: 1.1895 - acc: 0.6322 - val_loss: 0.8372 - val_acc: 0.7549\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 700s - loss: 1.1725 - acc: 0.6332 - val_loss: 0.9466 - val_acc: 0.7531\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 702s - loss: 1.1527 - acc: 0.6413 - val_loss: 0.8056 - val_acc: 0.7673\n",
      "Epoch 5/6\n",
      "2048/3214 [==================>...........] - ETA: 220s - loss: 1.0884 - acc: 0.6572"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(modelpath+'/custom3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Simpler arch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_13 (BatchNorm (None, 3, 640, 360)   12          batchnormalization_input_4[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 64, 640, 360)  1792        batchnormalization_13[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_10 (MaxPooling2D)   (None, 64, 320, 180)  0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_14 (BatchNorm (None, 64, 320, 180)  256         maxpooling2d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 64, 320, 180)  0           batchnormalization_14[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 64, 320, 180)  36928       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_11 (MaxPooling2D)   (None, 64, 160, 90)   0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_15 (BatchNorm (None, 64, 160, 90)   256         maxpooling2d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 64, 160, 90)   0           batchnormalization_15[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 64, 160, 90)   36928       dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_12 (MaxPooling2D)   (None, 64, 80, 45)    0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_16 (BatchNorm (None, 64, 80, 45)    256         maxpooling2d_12[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 64, 80, 45)    0           batchnormalization_16[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 230400)        0           dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 256)           58982656    flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 256)           0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 8)             2056        dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 59,061,140\n",
      "Trainable params: 59,060,750\n",
      "Non-trainable params: 390\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "path = 'data/fish'\n",
    "trainpath = path + '/train'\n",
    "validpath = path + '/valid'\n",
    "models_path = path+'/models/'\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.core import Lambda, Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "def convblock(model, layers, filters):\n",
    "    for _ in xrange(0, layers):\n",
    "        model.add(Convolution2D(filters, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(axis=1, input_shape=(3, 640, 360)))\n",
    "convblock(model, 1, 64)\n",
    "model.add(Dropout(0.1))\n",
    "convblock(model, 1, 64)\n",
    "model.add(Dropout(0.1))\n",
    "convblock(model, 1, 64)\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def batch_gen(path, class_mode='categorical', shuffle=True, rotation_range=0., zoom_range=0., channel_shift_range=0., horizontal_flip=False, width_shift_range=0., height_shift_range=0., batch_size=4):\n",
    "    return ImageDataGenerator(rotation_range=rotation_range, zoom_range=zoom_range, channel_shift_range=channel_shift_range, horizontal_flip=horizontal_flip, width_shift_range=width_shift_range, height_shift_range=height_shift_range).flow_from_directory(path, target_size=(640, 360), batch_size=batch_size, class_mode=class_mode, shuffle=shuffle)\n",
    "\n",
    "def image_aug(path, batch_size=4):\n",
    "    return batch_gen(path, zoom_range=0.10, channel_shift_range=0.10, horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(trainpath, batch_size=16)\n",
    "valid_gen = batch_gen(validpath, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3214/3214 [==============================] - 215s - loss: 2.0610 - acc: 0.4919 - val_loss: 2.5846 - val_acc: 0.4565\n",
      "Epoch 2/2\n",
      "3214/3214 [==============================] - 211s - loss: 1.4702 - acc: 0.5865 - val_loss: 1.4680 - val_acc: 0.5417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3066ff98>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.optimizer.lr = 0.00001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=2, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 221s - loss: 1.2986 - acc: 0.6232 - val_loss: 0.9922 - val_acc: 0.6909\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 210s - loss: 1.1334 - acc: 0.6649 - val_loss: 0.9085 - val_acc: 0.6643\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 211s - loss: 1.0204 - acc: 0.6886 - val_loss: 0.9947 - val_acc: 0.7194\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 214s - loss: 0.9301 - acc: 0.7038 - val_loss: 0.7938 - val_acc: 0.7318\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 211s - loss: 0.8505 - acc: 0.7324 - val_loss: 0.7846 - val_acc: 0.7460\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 210s - loss: 0.7990 - acc: 0.7365 - val_loss: 0.8452 - val_acc: 0.7425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x312d36d8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler2_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 214s - loss: 0.7373 - acc: 0.7598 - val_loss: 0.9518 - val_acc: 0.7034\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 207s - loss: 0.7495 - acc: 0.7710 - val_loss: 0.7078 - val_acc: 0.7833\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 210s - loss: 0.7240 - acc: 0.7741 - val_loss: 0.9394 - val_acc: 0.7140\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 209s - loss: 0.6510 - acc: 0.7931 - val_loss: 0.7124 - val_acc: 0.7620\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 208s - loss: 0.6503 - acc: 0.8015 - val_loss: 0.8321 - val_acc: 0.7425\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 206s - loss: 0.6082 - acc: 0.8074 - val_loss: 0.7398 - val_acc: 0.7584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x312e1c18>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler2_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 219s - loss: 0.6113 - acc: 0.8111 - val_loss: 0.7454 - val_acc: 0.7496\n",
      "Epoch 2/6\n",
      "2240/3214 [===================>..........] - ETA: 53s - loss: 0.5733 - acc: 0.8152"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-256f0e07fa3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.00001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n\u001b[1;32m----> 3\u001b[1;33m                     validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda2\\envs\\fastai\\lib\\site-packages\\keras\\models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    933\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1555\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1556\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1557\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1559\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1318\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1321\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\fastai\\lib\\site-packages\\keras\\backend\\theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 959\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    960\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\fastai\\lib\\site-packages\\theano\\compile\\function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\fastai\\lib\\site-packages\\theano\\ifelse.pyc\u001b[0m in \u001b[0;36mthunk\u001b[1;34m()\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m         \u001b[1;32mdef\u001b[0m \u001b[0mthunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 0.00001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpler architecture again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_1 (BatchNorma (None, 3, 640, 360)   12          batchnormalization_input_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 640, 360)  1792        batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 320, 180)  0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 64, 320, 180)  256         maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 128, 320, 180) 73856       batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 160, 90)  0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 128, 160, 90)  512         maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 160, 90)  147584      batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 128, 80, 45)   0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 128, 80, 45)   512         maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 460800)        0           batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 64)            29491264    flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 8)             520         dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 29,716,308\n",
      "Trainable params: 29,715,662\n",
      "Non-trainable params: 646\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "path = 'data/fish'\n",
    "trainpath = path + '/train'\n",
    "validpath = path + '/valid'\n",
    "models_path = path+'/models/'\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.core import Lambda, Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "def convblock(model, layers, filters):\n",
    "    for _ in xrange(0, layers):\n",
    "        model.add(Convolution2D(filters, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(axis=1, input_shape=(3, 640, 360)))\n",
    "convblock(model, 1, 64)\n",
    "convblock(model, 1, 128)\n",
    "convblock(model, 1, 128)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "    #Dense(4096, activation='relu'),\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def batch_gen(path, class_mode='categorical', shuffle=True, rotation_range=0., zoom_range=0., channel_shift_range=0., horizontal_flip=False, width_shift_range=0., height_shift_range=0., batch_size=4):\n",
    "    return ImageDataGenerator(rotation_range=rotation_range, zoom_range=zoom_range, channel_shift_range=channel_shift_range, horizontal_flip=horizontal_flip, width_shift_range=width_shift_range, height_shift_range=height_shift_range).flow_from_directory(path, target_size=(640, 360), batch_size=batch_size, class_mode=class_mode, shuffle=shuffle)\n",
    "\n",
    "def image_aug(path, batch_size=4):\n",
    "    return batch_gen(path, zoom_range=0.10, channel_shift_range=0.10, horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05, batch_size=batch_size, rotation_range=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(trainpath, batch_size=16)\n",
    "valid_gen = batch_gen(validpath, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3214/3214 [==============================] - 219s - loss: 1.7348 - acc: 0.4564 - val_loss: 1.9861 - val_acc: 0.3357\n",
      "Epoch 2/2\n",
      "3214/3214 [==============================] - 216s - loss: 1.3947 - acc: 0.5314 - val_loss: 1.4687 - val_acc: 0.5062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e3d4d68>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.optimizer.lr = 0.00001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=2, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path + '/models/simpler3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 224s - loss: 1.2669 - acc: 0.5635 - val_loss: 1.3074 - val_acc: 0.5204\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 209s - loss: 1.2479 - acc: 0.5713 - val_loss: 1.1722 - val_acc: 0.5631\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 209s - loss: 1.1467 - acc: 0.6033 - val_loss: 1.1465 - val_acc: 0.5222\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 210s - loss: 1.0891 - acc: 0.6207 - val_loss: 1.1124 - val_acc: 0.6039\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 214s - loss: 1.0654 - acc: 0.6232 - val_loss: 1.0111 - val_acc: 0.6217\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 212s - loss: 1.0423 - acc: 0.6431 - val_loss: 0.9338 - val_acc: 0.6483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5acda240>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path + '/models/simpler3_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 219s - loss: 0.9596 - acc: 0.6559 - val_loss: 0.8926 - val_acc: 0.6714\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 208s - loss: 0.9693 - acc: 0.6500 - val_loss: 0.8692 - val_acc: 0.7052\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 212s - loss: 0.9343 - acc: 0.6717 - val_loss: 0.9887 - val_acc: 0.6874\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 213s - loss: 0.9100 - acc: 0.6752 - val_loss: 0.9572 - val_acc: 0.6483\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 207s - loss: 0.8910 - acc: 0.6758 - val_loss: 0.8896 - val_acc: 0.6856\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 210s - loss: 0.8653 - acc: 0.6845 - val_loss: 0.8325 - val_acc: 0.6785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5acdaf98>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.00001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path + '/models/simpler3_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 896/3214 [=======>......................] - ETA: 138s - loss: 0.8881 - acc: 0.6775"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-732b1a0dcf71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=2, \n\u001b[1;32m----> 3\u001b[1;33m                     validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda2\\envs\\fastai\\lib\\site-packages\\keras\\models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    933\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1555\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1556\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1557\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1559\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1318\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1321\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\fastai\\lib\\site-packages\\keras\\backend\\theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 959\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    960\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\fastai\\lib\\site-packages\\theano\\compile\\function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 0.01\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=2, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1080 Ti (CNMeM is disabled, cuDNN 5110)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_1 (BatchNorma (None, 3, 640, 360)   12          batchnormalization_input_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 128, 640, 360) 3584        batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 128, 320, 180) 0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 128, 320, 180) 512         maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 128, 320, 180) 147584      batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 160, 90)  0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 128, 160, 90)  512         maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 160, 90)  147584      batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 128, 80, 45)   0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 128, 80, 45)   512         maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 460800)        0           batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           58982528    flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 8)             1032        dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 59,283,860\n",
      "Trainable params: 59,283,086\n",
      "Non-trainable params: 774\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "path = 'data/fish'\n",
    "trainpath = path + '/train'\n",
    "validpath = path + '/valid'\n",
    "models_path = path+'/models/'\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.core import Lambda, Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "def convblock(model, layers, filters):\n",
    "    for _ in xrange(0, layers):\n",
    "        model.add(Convolution2D(filters, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(axis=1, input_shape=(3, 640, 360)))\n",
    "convblock(model, 1, 128)\n",
    "convblock(model, 1, 128)\n",
    "convblock(model, 1, 128)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def batch_gen(path, class_mode='categorical', shuffle=True, rotation_range=0., zoom_range=0., channel_shift_range=0., horizontal_flip=False, width_shift_range=0., height_shift_range=0., batch_size=4):\n",
    "    return ImageDataGenerator(rotation_range=rotation_range, zoom_range=zoom_range, channel_shift_range=channel_shift_range, horizontal_flip=horizontal_flip, width_shift_range=width_shift_range, height_shift_range=height_shift_range).flow_from_directory(path, target_size=(640, 360), batch_size=batch_size, class_mode=class_mode, shuffle=shuffle)\n",
    "\n",
    "def image_aug(path, batch_size=4):\n",
    "    return batch_gen(path, zoom_range=0.10, channel_shift_range=0.10, horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05, batch_size=batch_size, rotation_range=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(trainpath, batch_size=16)\n",
    "valid_gen = batch_gen(validpath, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3214/3214 [==============================] - 247s - loss: 2.0024 - acc: 0.4428 - val_loss: 2.1852 - val_acc: 0.3694\n",
      "Epoch 2/2\n",
      "3214/3214 [==============================] - 236s - loss: 1.5298 - acc: 0.5227 - val_loss: 1.7632 - val_acc: 0.4352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2f549160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.optimizer.lr = 0.00001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=2, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler128.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 248s - loss: 1.3212 - acc: 0.5719 - val_loss: 1.3673 - val_acc: 0.5293\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 244s - loss: 1.2056 - acc: 0.5881 - val_loss: 1.2215 - val_acc: 0.5897\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 237s - loss: 1.1686 - acc: 0.6083 - val_loss: 1.3588 - val_acc: 0.6181\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 239s - loss: 1.0760 - acc: 0.6266 - val_loss: 1.3255 - val_acc: 0.5631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x454b51d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.01\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler128_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 250s - loss: 1.0628 - acc: 0.6332 - val_loss: 1.0638 - val_acc: 0.6270\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 242s - loss: 1.0374 - acc: 0.6413 - val_loss: 0.9757 - val_acc: 0.6821\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 244s - loss: 0.9774 - acc: 0.6668 - val_loss: 1.0688 - val_acc: 0.6767\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 245s - loss: 0.9655 - acc: 0.6705 - val_loss: 0.8729 - val_acc: 0.6856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x454b8ef0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler128_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 251s - loss: 0.9493 - acc: 0.6770 - val_loss: 0.8875 - val_acc: 0.7211\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 243s - loss: 0.9019 - acc: 0.6861 - val_loss: 0.9552 - val_acc: 0.6554\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 259s - loss: 0.8380 - acc: 0.6966 - val_loss: 0.9574 - val_acc: 0.7069\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 257s - loss: 0.8668 - acc: 0.6979 - val_loss: 0.7748 - val_acc: 0.7353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x454bae80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler128_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpler again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1080 Ti (CNMeM is disabled, cuDNN 5110)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_1 (BatchNorma (None, 3, 640, 360)   12          batchnormalization_input_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 640, 360)  1792        batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 320, 180)  0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 64, 320, 180)  256         maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 320, 180)  36928       batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 64, 160, 90)   0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 64, 160, 90)   256         maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 64, 160, 90)   36928       batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 64, 80, 45)    0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 64, 80, 45)    256         maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 230400)        0           batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 64)            14745664    flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 8)             520         dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 14,822,612\n",
      "Trainable params: 14,822,222\n",
      "Non-trainable params: 390\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "path = 'data/fish'\n",
    "trainpath = path + '/train'\n",
    "validpath = path + '/valid'\n",
    "models_path = path+'/models/'\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.core import Lambda, Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "def convblock(model, layers, filters):\n",
    "    for _ in xrange(0, layers):\n",
    "        model.add(Convolution2D(filters, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(axis=1, input_shape=(3, 640, 360)))\n",
    "convblock(model, 1, 64)\n",
    "convblock(model, 1, 64)\n",
    "convblock(model, 1, 64)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "    #Dense(4096, activation='relu'),\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def batch_gen(path, class_mode='categorical', shuffle=True, rotation_range=0., zoom_range=0., channel_shift_range=0., horizontal_flip=False, width_shift_range=0., height_shift_range=0., batch_size=4):\n",
    "    return ImageDataGenerator(rotation_range=rotation_range, zoom_range=zoom_range, channel_shift_range=channel_shift_range, horizontal_flip=horizontal_flip, width_shift_range=width_shift_range, height_shift_range=height_shift_range).flow_from_directory(path, target_size=(640, 360), batch_size=batch_size, class_mode=class_mode, shuffle=shuffle)\n",
    "\n",
    "def image_aug(path, batch_size=4):\n",
    "    return batch_gen(path, zoom_range=0.10, channel_shift_range=0.10, horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05, batch_size=batch_size, rotation_range=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(trainpath, batch_size=16)\n",
    "valid_gen = batch_gen(validpath, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3214/3214 [==============================] - 221s - loss: 1.7070 - acc: 0.4543 - val_loss: 1.7060 - val_acc: 0.3961\n",
      "Epoch 2/2\n",
      "3214/3214 [==============================] - 210s - loss: 1.4992 - acc: 0.4997 - val_loss: 1.5699 - val_acc: 0.4547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x31a1a748>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.optimizer.lr = 0.00001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=2, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler_more_data_aug.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 217s - loss: 1.3368 - acc: 0.5348 - val_loss: 1.3381 - val_acc: 0.5595\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 209s - loss: 1.2586 - acc: 0.5647 - val_loss: 1.2854 - val_acc: 0.5702\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 210s - loss: 1.1925 - acc: 0.5912 - val_loss: 1.2417 - val_acc: 0.5861\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 211s - loss: 1.1536 - acc: 0.6033 - val_loss: 1.2387 - val_acc: 0.5560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x50234748>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler_more_data_aug_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 213s - loss: 1.1397 - acc: 0.6114 - val_loss: 1.1128 - val_acc: 0.5808\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 202s - loss: 1.1102 - acc: 0.6161 - val_loss: 1.1921 - val_acc: 0.6341\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 203s - loss: 1.0759 - acc: 0.6170 - val_loss: 1.0390 - val_acc: 0.6181\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 204s - loss: 1.0561 - acc: 0.6276 - val_loss: 0.9030 - val_acc: 0.6536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5023c588>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.01\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler_more_data_aug_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 213s - loss: 1.0046 - acc: 0.6453 - val_loss: 0.9111 - val_acc: 0.6661\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 211s - loss: 0.9729 - acc: 0.6512 - val_loss: 0.9372 - val_acc: 0.6554\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 205s - loss: 0.9459 - acc: 0.6658 - val_loss: 0.9857 - val_acc: 0.6448\n",
      "Epoch 4/4\n",
      "3198/3214 [============================>.] - ETA: 0s - loss: 0.9138 - acc: 0.6807"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler_more_data_aug_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "testpath = path + '/test_stg1'\n",
    "test_gen = batch_gen(testpath, class_mode=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_one = model.predict_generator(test_gen, test_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_00005.jpg</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_00007.jpg</td>\n",
       "      <td>0.409420</td>\n",
       "      <td>0.012094</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.474855</td>\n",
       "      <td>0.043222</td>\n",
       "      <td>0.058334</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_00009.jpg</td>\n",
       "      <td>0.775162</td>\n",
       "      <td>0.016303</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.200159</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_00018.jpg</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.018517</td>\n",
       "      <td>0.010604</td>\n",
       "      <td>0.272065</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.292219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_00027.jpg</td>\n",
       "      <td>0.502995</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.016810</td>\n",
       "      <td>0.047343</td>\n",
       "      <td>0.010818</td>\n",
       "      <td>0.414221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image       ALB       BET       DOL       LAG       NoF     OTHER  \\\n",
       "0  img_00005.jpg  0.010000  0.010000  0.010000  0.010000  0.920000  0.010000   \n",
       "1  img_00007.jpg  0.409420  0.012094  0.010000  0.474855  0.043222  0.058334   \n",
       "2  img_00009.jpg  0.775162  0.016303  0.010000  0.010000  0.010000  0.200159   \n",
       "3  img_00018.jpg  0.374302  0.018517  0.010604  0.272065  0.010000  0.024913   \n",
       "4  img_00027.jpg  0.502995  0.010000  0.010000  0.010000  0.016810  0.047343   \n",
       "\n",
       "      SHARK       YFT  \n",
       "0  0.010000  0.010000  \n",
       "1  0.010000  0.010000  \n",
       "2  0.010000  0.010000  \n",
       "3  0.010000  0.292219  \n",
       "4  0.010818  0.414221  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/8, mx)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "classes = sorted(train_gen.class_indices, key=train_gen.class_indices.get)\n",
    "submission_one = pd.DataFrame(do_clip(predictions_one, 0.92), columns=classes)\n",
    "submission_one.insert(0, 'image', [os.path.basename(filename) for filename in test_gen.filenames])\n",
    "submission_one.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "testpath = path + '/test_stg2'\n",
    "test_gen = batch_gen(testpath, class_mode=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_two = model.predict_generator(test_gen, test_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_stg2/image_00001.jpg</td>\n",
       "      <td>0.036877</td>\n",
       "      <td>0.028955</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.338641</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.159242</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.434715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_stg2/image_00002.jpg</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_stg2/image_00003.jpg</td>\n",
       "      <td>0.328380</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.648453</td>\n",
       "      <td>0.021549</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_stg2/image_00004.jpg</td>\n",
       "      <td>0.231518</td>\n",
       "      <td>0.045085</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.221323</td>\n",
       "      <td>0.354097</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.139546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_stg2/image_00005.jpg</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.017282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image       ALB       BET   DOL       LAG       NoF  \\\n",
       "0  test_stg2/image_00001.jpg  0.036877  0.028955  0.01  0.338641  0.010000   \n",
       "1  test_stg2/image_00002.jpg  0.010000  0.010000  0.01  0.010000  0.920000   \n",
       "2  test_stg2/image_00003.jpg  0.328380  0.010000  0.01  0.010000  0.648453   \n",
       "3  test_stg2/image_00004.jpg  0.231518  0.045085  0.01  0.221323  0.354097   \n",
       "4  test_stg2/image_00005.jpg  0.010000  0.010000  0.01  0.010000  0.920000   \n",
       "\n",
       "      OTHER  SHARK       YFT  \n",
       "0  0.159242   0.01  0.434715  \n",
       "1  0.010000   0.01  0.010000  \n",
       "2  0.021549   0.01  0.010000  \n",
       "3  0.010000   0.01  0.139546  \n",
       "4  0.010000   0.01  0.017282  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = sorted(train_gen.class_indices, key=train_gen.class_indices.get)\n",
    "submission_two = pd.DataFrame(do_clip(predictions_two, 0.92), columns=classes)\n",
    "submission_two.insert(0, 'image', ['test_stg2/' + os.path.basename(filename) for filename in test_gen.filenames])\n",
    "submission_two.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = submission_one.append(submission_two, ignore_index=True)\n",
    "submission.to_csv(os.path.join(path, 'stage_two_custom_clipping_augments.csv.gz'), index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_preds = submission_two.sample(frac=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "pseudotrainpath = path + '/pseudotrain'\n",
    "if not os.path.exists(pseudotrainpath):\n",
    "    os.makedirs(pseudotrainpath)\n",
    "\n",
    "for pred in sample_preds.itertuples(index=False):\n",
    "    folder = sample_preds.columns[np.argmax(pred[1:]) + 1]\n",
    "    if not os.path.exists(os.path.join(pseudotrainpath, folder)):\n",
    "        os.makedirs(os.path.join(pseudotrainpath, folder))\n",
    "    shutil.copyfile(path + '/test_stg2/unknown/' + os.path.basename(pred[0]), pseudotrainpath + '/' + folder + '/' + os.path.basename(pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1823 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(pseudotrainpath, batch_size=16)\n",
    "valid_gen = batch_gen(validpath, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1823/1823 [==============================] - 129s - loss: 0.8927 - acc: 0.7224 - val_loss: 1.4565 - val_acc: 0.5737\n",
      "Epoch 2/2\n",
      "1823/1823 [==============================] - 119s - loss: 0.7880 - acc: 0.7570 - val_loss: 1.5731 - val_acc: 0.5915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x532a3748>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=2, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(trainpath, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "2352/3214 [====================>.........] - ETA: 50s - loss: 1.0433 - acc: 0.6471"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-7283ff2e2930>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n\u001b[1;32m----> 3\u001b[1;33m                     validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda2\\envs\\fastai\\lib\\site-packages\\keras\\models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    933\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1555\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1556\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1557\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1559\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1318\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1321\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\fastai\\lib\\site-packages\\keras\\backend\\theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 959\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    960\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\fastai\\lib\\site-packages\\theano\\compile\\function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler_more_data_aug_5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More traditional design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1080 Ti (CNMeM is disabled, cuDNN 5110)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_1 (BatchNorma (None, 3, 640, 360)   12          batchnormalization_input_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 640, 360)  1792        batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 640, 360)  36928       convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 64, 640, 360)  36928       convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 320, 180)  0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 64, 320, 180)  256         maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 320, 180) 73856       batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 128, 320, 180) 147584      convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 128, 320, 180) 147584      convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 160, 90)  0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 128, 160, 90)  512         maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 256, 160, 90)  295168      batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 256, 160, 90)  590080      convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 256, 160, 90)  590080      convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 256, 80, 45)   0           convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 256, 80, 45)   1024        maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 128, 80, 45)   295040      batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 128, 80, 45)   147584      convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 128, 40, 22)   0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_5 (BatchNorma (None, 128, 40, 22)   512         maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 64, 40, 22)    73792       batchnormalization_5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 64, 40, 22)    36928       convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 64, 20, 11)    0           convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_6 (BatchNorma (None, 64, 20, 11)    256         maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 14080)         0           batchnormalization_6[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1024)          14418944    flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_7 (BatchNorma (None, 1024)          4096        dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 128)           131200      batchnormalization_7[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_8 (BatchNorma (None, 128)           512         dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 8)             1032        batchnormalization_8[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 17,031,700\n",
      "Trainable params: 17,028,110\n",
      "Non-trainable params: 3,590\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "path = 'data/fish'\n",
    "trainpath = path + '/train'\n",
    "samplepath = path + '/sample'\n",
    "validpath = path + '/valid'\n",
    "models_path = path+'/models/'\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.core import Lambda, Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "def convblock(model, layers, filters):\n",
    "    for _ in xrange(0, layers):\n",
    "        model.add(Convolution2D(filters, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(axis=1, input_shape=(3, 640, 360)))\n",
    "convblock(model, 3, 64)\n",
    "convblock(model, 3, 128)\n",
    "convblock(model, 3, 256)\n",
    "convblock(model, 2, 128)\n",
    "convblock(model, 2, 64)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def batch_gen(path, class_mode='categorical', shuffle=True, rotation_range=0., zoom_range=0., channel_shift_range=0., horizontal_flip=False, width_shift_range=0., height_shift_range=0., batch_size=4):\n",
    "    return ImageDataGenerator(rotation_range=rotation_range, zoom_range=zoom_range, channel_shift_range=channel_shift_range, horizontal_flip=horizontal_flip, width_shift_range=width_shift_range, height_shift_range=height_shift_range).flow_from_directory(path, target_size=(640, 360), batch_size=batch_size, class_mode=class_mode, shuffle=shuffle)\n",
    "\n",
    "def image_aug(path, batch_size=4):\n",
    "    return batch_gen(path, zoom_range=0.10, channel_shift_range=0.10, horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05, batch_size=batch_size, rotation_range=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(trainpath, batch_size=8)\n",
    "valid_gen = batch_gen(validpath, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3214/3214 [==============================] - 534s - loss: 2.5300 - acc: 0.1826 - val_loss: 2.3963 - val_acc: 0.2842\n",
      "Epoch 2/2\n",
      "3214/3214 [==============================] - 533s - loss: 2.2398 - acc: 0.2505 - val_loss: 2.3369 - val_acc: 0.1901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x566d3cc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.00001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=2, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/traditional.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 533s - loss: 2.1312 - acc: 0.2825 - val_loss: 1.9413 - val_acc: 0.2789\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 532s - loss: 2.0010 - acc: 0.3267 - val_loss: 1.9056 - val_acc: 0.3517\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 532s - loss: 1.9225 - acc: 0.3485 - val_loss: 1.7357 - val_acc: 0.3766\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 532s - loss: 1.8630 - acc: 0.3855 - val_loss: 1.9722 - val_acc: 0.3535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5886f2e8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/traditional_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 530s - loss: 1.9487 - acc: 0.3398 - val_loss: 1.8662 - val_acc: 0.4174\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 529s - loss: 1.6554 - acc: 0.4269 - val_loss: 1.5940 - val_acc: 0.4618\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 528s - loss: 1.4859 - acc: 0.5028 - val_loss: 1.4409 - val_acc: 0.5098\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 531s - loss: 1.3968 - acc: 0.5233 - val_loss: 1.3789 - val_acc: 0.5222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5660f9b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(path+'/models/traditional_2.h5')\n",
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/traditional_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3214/3214 [==============================] - 531s - loss: 1.2523 - acc: 0.5800 - val_loss: 1.2148 - val_acc: 0.5897\n",
      "Epoch 2/3\n",
      "3214/3214 [==============================] - 532s - loss: 1.2292 - acc: 0.5837 - val_loss: 1.1398 - val_acc: 0.6163\n",
      "Epoch 3/3\n",
      "3214/3214 [==============================] - 531s - loss: 1.1927 - acc: 0.5977 - val_loss: 1.1345 - val_acc: 0.6270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3be16940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(path+'/models/traditional_3.h5')\n",
    "model.optimizer.lr = 0.00001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=3, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/traditional_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3214/3214 [==============================] - 526s - loss: 1.1466 - acc: 0.6133 - val_loss: 1.0929 - val_acc: 0.6217\n",
      "Epoch 2/3\n",
      "3214/3214 [==============================] - 528s - loss: 1.1420 - acc: 0.6123 - val_loss: 1.0561 - val_acc: 0.6501\n",
      "Epoch 3/3\n",
      "3214/3214 [==============================] - 529s - loss: 1.1075 - acc: 0.6226 - val_loss: 1.0842 - val_acc: 0.6767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x57571470>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=3, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/traditional_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3214/3214 [==============================] - 529s - loss: 1.0886 - acc: 0.6297 - val_loss: 1.0580 - val_acc: 0.6732\n",
      "Epoch 2/3\n",
      "3214/3214 [==============================] - 528s - loss: 1.0472 - acc: 0.6509 - val_loss: 0.9667 - val_acc: 0.6874\n",
      "Epoch 3/3\n",
      "3214/3214 [==============================] - 527s - loss: 1.0507 - acc: 0.6406 - val_loss: 0.9908 - val_acc: 0.6696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x575767f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=3, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/traditional_6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "testpath = path + '/test_stg1'\n",
    "test_gen = batch_gen(testpath, class_mode=None, shuffle=False)\n",
    "predictions_one = model.predict_generator(test_gen, test_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_00005.jpg</td>\n",
       "      <td>0.311465</td>\n",
       "      <td>0.034179</td>\n",
       "      <td>0.198956</td>\n",
       "      <td>0.031402</td>\n",
       "      <td>0.282189</td>\n",
       "      <td>0.101234</td>\n",
       "      <td>0.027297</td>\n",
       "      <td>0.013277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_00007.jpg</td>\n",
       "      <td>0.026094</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.024009</td>\n",
       "      <td>0.508718</td>\n",
       "      <td>0.012018</td>\n",
       "      <td>0.064890</td>\n",
       "      <td>0.063603</td>\n",
       "      <td>0.158168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_00009.jpg</td>\n",
       "      <td>0.205833</td>\n",
       "      <td>0.037878</td>\n",
       "      <td>0.012396</td>\n",
       "      <td>0.021625</td>\n",
       "      <td>0.513593</td>\n",
       "      <td>0.161785</td>\n",
       "      <td>0.015657</td>\n",
       "      <td>0.031234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_00018.jpg</td>\n",
       "      <td>0.730616</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.015934</td>\n",
       "      <td>0.042918</td>\n",
       "      <td>0.079380</td>\n",
       "      <td>0.026366</td>\n",
       "      <td>0.044917</td>\n",
       "      <td>0.048374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_00027.jpg</td>\n",
       "      <td>0.562964</td>\n",
       "      <td>0.040978</td>\n",
       "      <td>0.018738</td>\n",
       "      <td>0.016989</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>0.076207</td>\n",
       "      <td>0.198493</td>\n",
       "      <td>0.072088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image       ALB       BET       DOL       LAG       NoF     OTHER  \\\n",
       "0  img_00005.jpg  0.311465  0.034179  0.198956  0.031402  0.282189  0.101234   \n",
       "1  img_00007.jpg  0.026094  0.142500  0.024009  0.508718  0.012018  0.064890   \n",
       "2  img_00009.jpg  0.205833  0.037878  0.012396  0.021625  0.513593  0.161785   \n",
       "3  img_00018.jpg  0.730616  0.011494  0.015934  0.042918  0.079380  0.026366   \n",
       "4  img_00027.jpg  0.562964  0.040978  0.018738  0.016989  0.013542  0.076207   \n",
       "\n",
       "      SHARK       YFT  \n",
       "0  0.027297  0.013277  \n",
       "1  0.063603  0.158168  \n",
       "2  0.015657  0.031234  \n",
       "3  0.044917  0.048374  \n",
       "4  0.198493  0.072088  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/8, mx)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "classes = sorted(train_gen.class_indices, key=train_gen.class_indices.get)\n",
    "submission_one = pd.DataFrame(do_clip(predictions_one, 0.92), columns=classes)\n",
    "submission_one.insert(0, 'image', [os.path.basename(filename) for filename in test_gen.filenames])\n",
    "submission_one.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "testpath = path + '/test_stg2'\n",
    "test_gen = batch_gen(testpath, class_mode=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_two = model.predict_generator(test_gen, test_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_stg2/image_00001.jpg</td>\n",
       "      <td>0.010216</td>\n",
       "      <td>0.018376</td>\n",
       "      <td>0.015547</td>\n",
       "      <td>0.024640</td>\n",
       "      <td>0.014235</td>\n",
       "      <td>0.555381</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.351052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_stg2/image_00002.jpg</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_stg2/image_00003.jpg</td>\n",
       "      <td>0.095582</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.025407</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.254244</td>\n",
       "      <td>0.120992</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.487274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_stg2/image_00004.jpg</td>\n",
       "      <td>0.039727</td>\n",
       "      <td>0.164521</td>\n",
       "      <td>0.053337</td>\n",
       "      <td>0.117737</td>\n",
       "      <td>0.019850</td>\n",
       "      <td>0.033516</td>\n",
       "      <td>0.054372</td>\n",
       "      <td>0.516940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_stg2/image_00005.jpg</td>\n",
       "      <td>0.061531</td>\n",
       "      <td>0.089836</td>\n",
       "      <td>0.131566</td>\n",
       "      <td>0.090609</td>\n",
       "      <td>0.221638</td>\n",
       "      <td>0.062692</td>\n",
       "      <td>0.028299</td>\n",
       "      <td>0.313829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image       ALB       BET       DOL       LAG  \\\n",
       "0  test_stg2/image_00001.jpg  0.010216  0.018376  0.015547  0.024640   \n",
       "1  test_stg2/image_00002.jpg  0.010000  0.010000  0.010000  0.010000   \n",
       "2  test_stg2/image_00003.jpg  0.095582  0.010000  0.025407  0.010000   \n",
       "3  test_stg2/image_00004.jpg  0.039727  0.164521  0.053337  0.117737   \n",
       "4  test_stg2/image_00005.jpg  0.061531  0.089836  0.131566  0.090609   \n",
       "\n",
       "        NoF     OTHER     SHARK       YFT  \n",
       "0  0.014235  0.555381  0.010554  0.351052  \n",
       "1  0.920000  0.010000  0.010000  0.010000  \n",
       "2  0.254244  0.120992  0.010000  0.487274  \n",
       "3  0.019850  0.033516  0.054372  0.516940  \n",
       "4  0.221638  0.062692  0.028299  0.313829  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = sorted(train_gen.class_indices, key=train_gen.class_indices.get)\n",
    "submission_two = pd.DataFrame(do_clip(predictions_two, 0.92), columns=classes)\n",
    "submission_two.insert(0, 'image', ['test_stg2/' + os.path.basename(filename) for filename in test_gen.filenames])\n",
    "submission_two.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = submission_one.append(submission_two, ignore_index=True)\n",
    "submission.to_csv(os.path.join(path, 'stage_two_more_traditional.csv.gz'), index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3214/3214 [==============================] - 529s - loss: 1.0096 - acc: 0.6518 - val_loss: 0.9224 - val_acc: 0.7318\n",
      "Epoch 2/3\n",
      "3214/3214 [==============================] - 529s - loss: 1.0101 - acc: 0.6559 - val_loss: 1.0049 - val_acc: 0.6696\n",
      "Epoch 3/3\n",
      "3214/3214 [==============================] - 529s - loss: 0.9999 - acc: 0.6649 - val_loss: 0.9527 - val_acc: 0.6892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5d800e48>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=3, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/traditional_7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3214/3214 [==============================] - 529s - loss: 0.9844 - acc: 0.6500 - val_loss: 1.0392 - val_acc: 0.6732\n",
      "Epoch 2/3\n",
      "3214/3214 [==============================] - 528s - loss: 0.9828 - acc: 0.6699 - val_loss: 0.9233 - val_acc: 0.6856\n",
      "Epoch 3/3\n",
      "3214/3214 [==============================] - 529s - loss: 0.9474 - acc: 0.6854 - val_loss: 0.8566 - val_acc: 0.7300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5d8f6f60>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=3, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/traditional_8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 527s - loss: 0.9322 - acc: 0.6786 - val_loss: 0.8715 - val_acc: 0.7105\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 531s - loss: 0.9184 - acc: 0.6792 - val_loss: 0.9093 - val_acc: 0.6963\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 528s - loss: 0.8730 - acc: 0.7075 - val_loss: 0.8510 - val_acc: 0.7229\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 529s - loss: 0.9160 - acc: 0.6873 - val_loss: 0.8704 - val_acc: 0.7034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5d954358>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/traditional_9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 529s - loss: 0.8670 - acc: 0.7032 - val_loss: 0.8556 - val_acc: 0.7140\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 529s - loss: 0.8646 - acc: 0.7044 - val_loss: 0.8269 - val_acc: 0.7318\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 528s - loss: 0.8609 - acc: 0.7153 - val_loss: 0.8664 - val_acc: 0.6980\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 529s - loss: 0.8273 - acc: 0.7166 - val_loss: 0.8090 - val_acc: 0.7282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5d95b6a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/traditional_10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 529s - loss: 0.8351 - acc: 0.7113 - val_loss: 0.8547 - val_acc: 0.7176\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 530s - loss: 0.8188 - acc: 0.7172 - val_loss: 0.7886 - val_acc: 0.7318\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 529s - loss: 0.8058 - acc: 0.7290 - val_loss: 0.9097 - val_acc: 0.6998\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 528s - loss: 0.8050 - acc: 0.7281 - val_loss: 0.8594 - val_acc: 0.7282\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 528s - loss: 0.7749 - acc: 0.7455 - val_loss: 0.7660 - val_acc: 0.7780\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 534s - loss: 0.7871 - acc: 0.7371 - val_loss: 0.8570 - val_acc: 0.7336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5d95fa90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/traditional_11.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_submission(submission_path, clip=0.99):\n",
    "    testpath = path + '/test_stg1'\n",
    "    test_gen = batch_gen(testpath, class_mode=None, shuffle=False)\n",
    "    predictions_one = model.predict_generator(test_gen, test_gen.nb_sample)\n",
    "    \n",
    "    def do_clip(arr, mx): return np.clip(arr, (1-mx)/8, mx)\n",
    "\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    classes = sorted(train_gen.class_indices, key=train_gen.class_indices.get)\n",
    "    submission_one = pd.DataFrame(do_clip(predictions_one, clip), columns=classes)\n",
    "    submission_one.insert(0, 'image', [os.path.basename(filename) for filename in test_gen.filenames])\n",
    "    \n",
    "    testpath = path + '/test_stg2'\n",
    "    test_gen = batch_gen(testpath, class_mode=None, shuffle=False)\n",
    "    \n",
    "    predictions_two = model.predict_generator(test_gen, test_gen.nb_sample)\n",
    "    classes = sorted(train_gen.class_indices, key=train_gen.class_indices.get)\n",
    "    submission_two = pd.DataFrame(do_clip(predictions_two, 0.92), columns=classes)\n",
    "    submission_two.insert(0, 'image', ['test_stg2/' + os.path.basename(filename) for filename in test_gen.filenames])\n",
    "    \n",
    "    submission = submission_one.append(submission_two, ignore_index=True)\n",
    "    submission.to_csv(submission_path, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n",
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "generate_submission(path + '/more_training.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_19 (BatchNorm (None, 3, 640, 360)   12          batchnormalization_input_3[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_31 (Convolution2D) (None, 64, 640, 360)  1792        batchnormalization_19[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_32 (Convolution2D) (None, 64, 640, 360)  36928       convolution2d_31[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_13 (MaxPooling2D)   (None, 64, 320, 180)  0           convolution2d_32[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_20 (BatchNorm (None, 64, 320, 180)  256         maxpooling2d_13[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_33 (Convolution2D) (None, 128, 320, 180) 73856       batchnormalization_20[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_34 (Convolution2D) (None, 128, 320, 180) 147584      convolution2d_33[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_14 (MaxPooling2D)   (None, 128, 160, 90)  0           convolution2d_34[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_21 (BatchNorm (None, 128, 160, 90)  512         maxpooling2d_14[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_35 (Convolution2D) (None, 256, 160, 90)  295168      batchnormalization_21[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_36 (Convolution2D) (None, 256, 160, 90)  590080      convolution2d_35[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_37 (Convolution2D) (None, 256, 160, 90)  590080      convolution2d_36[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_15 (MaxPooling2D)   (None, 256, 80, 45)   0           convolution2d_37[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_22 (BatchNorm (None, 256, 80, 45)   1024        maxpooling2d_15[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_38 (Convolution2D) (None, 512, 80, 45)   1180160     batchnormalization_22[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_39 (Convolution2D) (None, 512, 80, 45)   2359808     convolution2d_38[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_40 (Convolution2D) (None, 512, 80, 45)   2359808     convolution2d_39[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_16 (MaxPooling2D)   (None, 512, 40, 22)   0           convolution2d_40[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_23 (BatchNorm (None, 512, 40, 22)   2048        maxpooling2d_16[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_41 (Convolution2D) (None, 1024, 40, 22)  4719616     batchnormalization_23[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_42 (Convolution2D) (None, 1024, 40, 22)  9438208     convolution2d_41[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_43 (Convolution2D) (None, 1024, 40, 22)  9438208     convolution2d_42[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_17 (MaxPooling2D)   (None, 1024, 20, 11)  0           convolution2d_43[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_24 (BatchNorm (None, 1024, 20, 11)  4096        maxpooling2d_17[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 225280)        0           batchnormalization_24[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 1024)          230687744   flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_25 (BatchNorm (None, 1024)          4096        dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 4096)          4198400     batchnormalization_25[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_26 (BatchNorm (None, 4096)          16384       dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 8)             32776       batchnormalization_26[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 266,178,644\n",
      "Trainable params: 266,164,430\n",
      "Non-trainable params: 14,214\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "path = 'data/fish'\n",
    "trainpath = path + '/train'\n",
    "samplepath = path + '/sample'\n",
    "validpath = path + '/valid'\n",
    "models_path = path+'/models/'\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.core import Lambda, Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "def convblock(model, layers, filters):\n",
    "    for _ in xrange(0, layers):\n",
    "        model.add(Convolution2D(filters, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(axis=1, input_shape=(3, 640, 360)))\n",
    "convblock(model, 2, 64)\n",
    "convblock(model, 2, 128)\n",
    "convblock(model, 3, 256)\n",
    "convblock(model, 3, 512)\n",
    "convblock(model, 3, 1024)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def batch_gen(path, class_mode='categorical', shuffle=True, rotation_range=0., zoom_range=0., channel_shift_range=0., horizontal_flip=False, width_shift_range=0., height_shift_range=0., batch_size=4):\n",
    "    return ImageDataGenerator(rotation_range=rotation_range, zoom_range=zoom_range, channel_shift_range=channel_shift_range, horizontal_flip=horizontal_flip, width_shift_range=width_shift_range, height_shift_range=height_shift_range).flow_from_directory(path, target_size=(640, 360), batch_size=batch_size, class_mode=class_mode, shuffle=shuffle)\n",
    "\n",
    "def image_aug(path, batch_size=4):\n",
    "    return batch_gen(path, zoom_range=0.10, channel_shift_range=0.10, horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05, batch_size=batch_size, rotation_range=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(trainpath, batch_size=8)\n",
    "valid_gen = batch_gen(validpath, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3214/3214 [==============================] - 589s - loss: 2.3192 - acc: 0.2931 - val_loss: 4.9259 - val_acc: 0.0568\n",
      "Epoch 2/2\n",
      "3214/3214 [==============================] - 587s - loss: 1.9946 - acc: 0.3696 - val_loss: 2.2069 - val_acc: 0.2984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2eae21d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.optimizer.lr = 0.00001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=2, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/deeper_traditional.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 593s - loss: 1.8158 - acc: 0.4113 - val_loss: 2.1204 - val_acc: 0.3339\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 590s - loss: 1.6921 - acc: 0.4540 - val_loss: 1.8893 - val_acc: 0.4618\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 589s - loss: 1.5819 - acc: 0.4745 - val_loss: 1.6717 - val_acc: 0.4529\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 588s - loss: 1.5374 - acc: 0.4960 - val_loss: 1.7966 - val_acc: 0.4440\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 594s - loss: 1.4494 - acc: 0.5115 - val_loss: 1.7104 - val_acc: 0.4529\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 590s - loss: 1.4119 - acc: 0.5289 - val_loss: 1.4674 - val_acc: 0.5240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa87c9cf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/deeper_traditional2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 590s - loss: 1.3628 - acc: 0.5451 - val_loss: 1.4120 - val_acc: 0.5346\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 590s - loss: 1.3190 - acc: 0.5579 - val_loss: 1.3246 - val_acc: 0.5471\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 591s - loss: 1.2459 - acc: 0.5837 - val_loss: 1.4615 - val_acc: 0.4991\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 592s - loss: 1.2792 - acc: 0.5663 - val_loss: 1.3451 - val_acc: 0.5222\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 592s - loss: 1.1709 - acc: 0.5971 - val_loss: 1.2219 - val_acc: 0.6092\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 590s - loss: 1.1836 - acc: 0.5993 - val_loss: 1.1922 - val_acc: 0.5986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa87d2f60>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/deeper_traditional3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(trainpath, batch_size=10)\n",
    "valid_gen = batch_gen(validpath, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 577s - loss: 1.0549 - acc: 0.6512 - val_loss: 1.0288 - val_acc: 0.6625\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 578s - loss: 1.0184 - acc: 0.6509 - val_loss: 1.0723 - val_acc: 0.6217\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 576s - loss: 0.9604 - acc: 0.6755 - val_loss: 0.9616 - val_acc: 0.6590\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 575s - loss: 0.9546 - acc: 0.6783 - val_loss: 0.9980 - val_acc: 0.6696\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 573s - loss: 0.9502 - acc: 0.6702 - val_loss: 0.9433 - val_acc: 0.6785\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 574s - loss: 0.9193 - acc: 0.6886 - val_loss: 0.9759 - val_acc: 0.6572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa880cba8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/deeper_traditional4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 574s - loss: 0.9239 - acc: 0.6892 - val_loss: 0.9049 - val_acc: 0.7016\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 574s - loss: 0.8953 - acc: 0.6973 - val_loss: 0.9296 - val_acc: 0.6803\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 573s - loss: 0.8612 - acc: 0.7072 - val_loss: 0.7698 - val_acc: 0.7709\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.8038 - acc: 0.7309 - val_loss: 0.8097 - val_acc: 0.6998\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.8060 - acc: 0.7278 - val_loss: 0.9738 - val_acc: 0.7016\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 574s - loss: 0.8203 - acc: 0.7321 - val_loss: 1.0241 - val_acc: 0.7123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa880df60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/deeper_traditional5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n",
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "generate_submission(path + '/deeper.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 574s - loss: 0.8200 - acc: 0.7172 - val_loss: 0.9886 - val_acc: 0.6838\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 574s - loss: 0.7507 - acc: 0.7486 - val_loss: 0.7503 - val_acc: 0.7265\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 573s - loss: 0.7406 - acc: 0.7536 - val_loss: 0.7902 - val_acc: 0.7496\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.7329 - acc: 0.7498 - val_loss: 0.7836 - val_acc: 0.7353\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.6839 - acc: 0.7654 - val_loss: 1.0554 - val_acc: 0.6643\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.6997 - acc: 0.7642 - val_loss: 0.8176 - val_acc: 0.7300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xac6f0278>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.00001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/deeper_traditional6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.7033 - acc: 0.7589 - val_loss: 0.8590 - val_acc: 0.7087\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.6549 - acc: 0.7797 - val_loss: 0.9136 - val_acc: 0.6821\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 574s - loss: 0.6375 - acc: 0.7866 - val_loss: 0.7236 - val_acc: 0.7602\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 573s - loss: 0.6848 - acc: 0.7735 - val_loss: 0.7624 - val_acc: 0.7460\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.6658 - acc: 0.7738 - val_loss: 0.7151 - val_acc: 0.7425\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.5887 - acc: 0.8002 - val_loss: 0.7230 - val_acc: 0.7940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xac6f0898>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/deeper_traditional7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.6035 - acc: 0.7946 - val_loss: 0.5210 - val_acc: 0.8242\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.6121 - acc: 0.7943 - val_loss: 0.8845 - val_acc: 0.7638\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 573s - loss: 0.5552 - acc: 0.8055 - val_loss: 0.6608 - val_acc: 0.7957\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 573s - loss: 0.5458 - acc: 0.8143 - val_loss: 0.7389 - val_acc: 0.7762\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 573s - loss: 0.5472 - acc: 0.8090 - val_loss: 0.7206 - val_acc: 0.7851\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 573s - loss: 0.5454 - acc: 0.8195 - val_loss: 0.4756 - val_acc: 0.8703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xac6f8ac8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/deeper_traditional8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 575s - loss: 0.5480 - acc: 0.8255 - val_loss: 0.5061 - val_acc: 0.8508\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 573s - loss: 0.5129 - acc: 0.8279 - val_loss: 0.6461 - val_acc: 0.7886\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.5266 - acc: 0.8189 - val_loss: 0.6117 - val_acc: 0.8153\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.4848 - acc: 0.8320 - val_loss: 0.6066 - val_acc: 0.8028\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.4791 - acc: 0.8382 - val_loss: 0.5505 - val_acc: 0.8242\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.4733 - acc: 0.8413 - val_loss: 0.5629 - val_acc: 0.8135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xac6fbfd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.000001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/deeper_traditional9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n",
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "generate_submission(path + '/deeper2.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 570s - loss: 0.4649 - acc: 0.8419 - val_loss: 0.5702 - val_acc: 0.8153\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.4386 - acc: 0.8500 - val_loss: 0.4714 - val_acc: 0.8313\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 570s - loss: 0.4623 - acc: 0.8488 - val_loss: 0.4747 - val_acc: 0.8472\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.4201 - acc: 0.8572 - val_loss: 0.4861 - val_acc: 0.8579\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 570s - loss: 0.4060 - acc: 0.8643 - val_loss: 0.4739 - val_acc: 0.8348\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 570s - loss: 0.4298 - acc: 0.8578 - val_loss: 0.4720 - val_acc: 0.8366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xacbffe80>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/deeper_traditional10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.4287 - acc: 0.8584 - val_loss: 0.5960 - val_acc: 0.8206\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.4313 - acc: 0.8544 - val_loss: 0.4054 - val_acc: 0.8544\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.4089 - acc: 0.8584 - val_loss: 0.4428 - val_acc: 0.8597\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.3675 - acc: 0.8740 - val_loss: 0.4577 - val_acc: 0.8739\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 570s - loss: 0.4016 - acc: 0.8675 - val_loss: 0.4426 - val_acc: 0.8579\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.3648 - acc: 0.8777 - val_loss: 0.3672 - val_acc: 0.8881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xac6ea320>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/deeper_traditional11.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.3873 - acc: 0.8662 - val_loss: 0.3856 - val_acc: 0.8757\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.3766 - acc: 0.8746 - val_loss: 0.3817 - val_acc: 0.8757\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.3336 - acc: 0.8927 - val_loss: 0.4456 - val_acc: 0.8544\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.3593 - acc: 0.8839 - val_loss: 0.3388 - val_acc: 0.9076\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 573s - loss: 0.3339 - acc: 0.8818 - val_loss: 0.3760 - val_acc: 0.8721\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 570s - loss: 0.3563 - acc: 0.8762 - val_loss: 0.4366 - val_acc: 0.8668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xacbfe6d8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/deeper_traditional12.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.3593 - acc: 0.8796 - val_loss: 0.3749 - val_acc: 0.8792\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.3497 - acc: 0.8821 - val_loss: 0.4507 - val_acc: 0.8544\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.3416 - acc: 0.8818 - val_loss: 0.3708 - val_acc: 0.8739\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.3291 - acc: 0.8871 - val_loss: 0.3313 - val_acc: 0.8863\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.2980 - acc: 0.8992 - val_loss: 0.4154 - val_acc: 0.8703\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.3022 - acc: 0.8998 - val_loss: 0.2876 - val_acc: 0.9165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xac702b38>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/deeper_traditional13.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 570s - loss: 0.3164 - acc: 0.8930 - val_loss: 0.4306 - val_acc: 0.8774\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.3388 - acc: 0.8855 - val_loss: 0.4038 - val_acc: 0.8774\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 569s - loss: 0.2896 - acc: 0.9042 - val_loss: 0.4601 - val_acc: 0.8686\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 573s - loss: 0.2927 - acc: 0.9014 - val_loss: 0.2830 - val_acc: 0.9094\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.2919 - acc: 0.9045 - val_loss: 0.3596 - val_acc: 0.8881\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.2736 - acc: 0.9126 - val_loss: 0.3003 - val_acc: 0.9112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xac7031d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/deeper_traditional14.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n",
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "generate_submission(path + '/deeper3.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.2917 - acc: 0.9067 - val_loss: 0.2395 - val_acc: 0.9236\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 573s - loss: 0.2717 - acc: 0.9070 - val_loss: 0.3165 - val_acc: 0.9059\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.2513 - acc: 0.9126 - val_loss: 0.2846 - val_acc: 0.9254\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.2704 - acc: 0.9026 - val_loss: 0.3617 - val_acc: 0.8810\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 570s - loss: 0.2529 - acc: 0.9194 - val_loss: 0.2994 - val_acc: 0.9183\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.2636 - acc: 0.9135 - val_loss: 0.3429 - val_acc: 0.9076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xacb971d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0000001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/deeper_traditional15.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 570s - loss: 0.2420 - acc: 0.9185 - val_loss: 0.2806 - val_acc: 0.9236\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 572s - loss: 0.2784 - acc: 0.9082 - val_loss: 0.3260 - val_acc: 0.9023\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.2661 - acc: 0.9060 - val_loss: 0.3105 - val_acc: 0.9165\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 567s - loss: 0.2537 - acc: 0.9194 - val_loss: 0.2506 - val_acc: 0.9272\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 570s - loss: 0.2627 - acc: 0.9110 - val_loss: 0.3492 - val_acc: 0.9147\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 571s - loss: 0.2183 - acc: 0.9238 - val_loss: 0.3247 - val_acc: 0.9094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb1ef6748>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0000001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/deeper_traditional16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n",
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "generate_submission(path + '/deeper4.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Try another approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1080 Ti (CNMeM is disabled, cuDNN 5110)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_1 (BatchNorma (None, 3, 320, 180)   12          batchnormalization_input_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 320, 180)  1792        batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 64, 320, 180)  256         convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 160, 90)   0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 128, 160, 90)  73856       maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 128, 160, 90)  512         convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 80, 45)   0           batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 460800)        0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           58982528    flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 128)           512         dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 8)             1032        batchnormalization_4[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 59,060,500\n",
      "Trainable params: 59,059,854\n",
      "Non-trainable params: 646\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "path = 'data/fish'\n",
    "trainpath = path + '/train'\n",
    "samplepath = path + '/sample'\n",
    "validpath = path + '/valid'\n",
    "models_path = path+'/models/'\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.core import Lambda, Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "def convblock(model, layers, filters):\n",
    "    for _ in xrange(0, layers):\n",
    "        model.add(Convolution2D(filters, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    #model.add(Dropout(0.1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(axis=1, input_shape=(3, 320, 180)))\n",
    "convblock(model, 1, 64)\n",
    "convblock(model, 1, 128)\n",
    "# convblock(model, 3, 256)\n",
    "# convblock(model, 3, 512)\n",
    "# convblock(model, 3, 512)\n",
    "# convblock(model, 3, 512)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(4096, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.load_weights(path+'/models/simpler.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def batch_gen(path, class_mode='categorical', shuffle=True, rotation_range=0., zoom_range=0., channel_shift_range=0., horizontal_flip=False, width_shift_range=0., height_shift_range=0., batch_size=4):\n",
    "    return ImageDataGenerator(rotation_range=rotation_range, zoom_range=zoom_range, channel_shift_range=channel_shift_range, horizontal_flip=horizontal_flip, width_shift_range=width_shift_range, height_shift_range=height_shift_range).flow_from_directory(path, target_size=(320, 180), batch_size=batch_size, class_mode=class_mode, shuffle=shuffle)\n",
    "\n",
    "def image_aug(path, batch_size=4):\n",
    "    return batch_gen(path, zoom_range=0.10, channel_shift_range=0.10, horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05, batch_size=batch_size, rotation_range=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 173 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = batch_gen(samplepath, batch_size=45)\n",
    "valid_gen = batch_gen(validpath, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "173/173 [==============================] - 18s - loss: 2.5351 - acc: 0.1618 - val_loss: 4.3884 - val_acc: 0.1758\n",
      "Epoch 2/6\n",
      "173/173 [==============================] - 15s - loss: 1.5864 - acc: 0.4624 - val_loss: 3.2970 - val_acc: 0.1829\n",
      "Epoch 3/6\n",
      "173/173 [==============================] - 15s - loss: 1.0412 - acc: 0.6590 - val_loss: 2.8770 - val_acc: 0.1812\n",
      "Epoch 4/6\n",
      "173/173 [==============================] - 15s - loss: 0.7192 - acc: 0.8092 - val_loss: 2.4520 - val_acc: 0.2309\n",
      "Epoch 5/6\n",
      "173/173 [==============================] - 15s - loss: 0.5196 - acc: 0.9133 - val_loss: 2.2656 - val_acc: 0.2167\n",
      "Epoch 6/6\n",
      "173/173 [==============================] - 15s - loss: 0.3966 - acc: 0.9364 - val_loss: 2.1282 - val_acc: 0.2327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x9395a390>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.000001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "173/173 [==============================] - 18s - loss: 0.2451 - acc: 0.9884 - val_loss: 2.0637 - val_acc: 0.2380\n",
      "Epoch 2/6\n",
      "173/173 [==============================] - 15s - loss: 0.2469 - acc: 0.9942 - val_loss: 2.0067 - val_acc: 0.2753\n",
      "Epoch 3/6\n",
      "173/173 [==============================] - 15s - loss: 0.1815 - acc: 1.0000 - val_loss: 2.0175 - val_acc: 0.2416\n",
      "Epoch 4/6\n",
      "173/173 [==============================] - 15s - loss: 0.1539 - acc: 1.0000 - val_loss: 1.9991 - val_acc: 0.2487\n",
      "Epoch 5/6\n",
      "173/173 [==============================] - 15s - loss: 0.1351 - acc: 1.0000 - val_loss: 1.9975 - val_acc: 0.2416\n",
      "Epoch 6/6\n",
      "173/173 [==============================] - 15s - loss: 0.1010 - acc: 1.0000 - val_loss: 2.0191 - val_acc: 0.2274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x9656cf98>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.000001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = batch_gen(trainpath, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 96s - loss: 0.8483 - acc: 0.7856 - val_loss: 1.6778 - val_acc: 0.4263\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 83s - loss: 0.2023 - acc: 0.9596 - val_loss: 1.7930 - val_acc: 0.3339\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 82s - loss: 0.0868 - acc: 0.9900 - val_loss: 1.7446 - val_acc: 0.3908\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 80s - loss: 0.0511 - acc: 0.9960 - val_loss: 1.3777 - val_acc: 0.5311\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 81s - loss: 0.0296 - acc: 0.9984 - val_loss: 1.0061 - val_acc: 0.7425\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 79s - loss: 0.0278 - acc: 0.9984 - val_loss: 0.5696 - val_acc: 0.8988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x31167b00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 92s - loss: 0.0186 - acc: 0.9988 - val_loss: 0.4410 - val_acc: 0.9218\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 80s - loss: 0.0177 - acc: 0.9981 - val_loss: 0.2951 - val_acc: 0.9538\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 80s - loss: 0.0189 - acc: 0.9984 - val_loss: 0.2587 - val_acc: 0.9503\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 82s - loss: 0.0204 - acc: 0.9981 - val_loss: 0.2411 - val_acc: 0.9591\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 81s - loss: 0.0234 - acc: 0.9988 - val_loss: 0.1914 - val_acc: 0.9645\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 81s - loss: 0.0106 - acc: 0.9997 - val_loss: 0.1556 - val_acc: 0.9751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4503aac8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 94s - loss: 0.0094 - acc: 0.9997 - val_loss: 0.2014 - val_acc: 0.9663\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 80s - loss: 0.0071 - acc: 0.9997 - val_loss: 0.1675 - val_acc: 0.9751\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 80s - loss: 0.0064 - acc: 0.9997 - val_loss: 0.1841 - val_acc: 0.9716\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 80s - loss: 0.0078 - acc: 0.9997 - val_loss: 0.1942 - val_acc: 0.9663\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 80s - loss: 0.0096 - acc: 0.9994 - val_loss: 0.1270 - val_acc: 0.9787\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 81s - loss: 0.0107 - acc: 0.9994 - val_loss: 0.1777 - val_acc: 0.9627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4503e978>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0000001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n",
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "generate_submission(path + '/simpler1.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(trainpath, batch_size=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 97s - loss: 2.0832 - acc: 0.4032 - val_loss: 1.1814 - val_acc: 0.7211\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 85s - loss: 1.6370 - acc: 0.5047 - val_loss: 1.1020 - val_acc: 0.6750\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 85s - loss: 1.5210 - acc: 0.5376 - val_loss: 1.0018 - val_acc: 0.7496\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 85s - loss: 1.4574 - acc: 0.5498 - val_loss: 1.0609 - val_acc: 0.7389\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 85s - loss: 1.3866 - acc: 0.5840 - val_loss: 0.9602 - val_acc: 0.7407\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 85s - loss: 1.3443 - acc: 0.5965 - val_loss: 1.0225 - val_acc: 0.7513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x460350f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 95s - loss: 1.2571 - acc: 0.6245 - val_loss: 1.0092 - val_acc: 0.7478\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 87s - loss: 1.2445 - acc: 0.6210 - val_loss: 0.8864 - val_acc: 0.7922\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 86s - loss: 1.1933 - acc: 0.6381 - val_loss: 1.0119 - val_acc: 0.7496\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 86s - loss: 1.1831 - acc: 0.6428 - val_loss: 0.9609 - val_acc: 0.7691\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 88s - loss: 1.1363 - acc: 0.6577 - val_loss: 0.9328 - val_acc: 0.7709\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 88s - loss: 1.1286 - acc: 0.6565 - val_loss: 0.8816 - val_acc: 0.7744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4601d0b8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.00001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_aug(path, batch_size=4):\n",
    "    return batch_gen(path, zoom_range=0.10, channel_shift_range=0.10, horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05, batch_size=batch_size, rotation_range=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 96s - loss: 1.0829 - acc: 0.6783 - val_loss: 0.9194 - val_acc: 0.7851\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 86s - loss: 1.0596 - acc: 0.6686 - val_loss: 0.8807 - val_acc: 0.7798\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 86s - loss: 1.0580 - acc: 0.6792 - val_loss: 0.8309 - val_acc: 0.7904\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 86s - loss: 1.0080 - acc: 0.6910 - val_loss: 0.6645 - val_acc: 0.8348\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 85s - loss: 1.0154 - acc: 0.6882 - val_loss: 0.8041 - val_acc: 0.7940\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 85s - loss: 1.0082 - acc: 0.6920 - val_loss: 0.7766 - val_acc: 0.8206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4601d048>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3214/3214 [==============================] - 97s - loss: 0.9717 - acc: 0.7019 - val_loss: 0.9038 - val_acc: 0.7549\n",
      "Epoch 2/3\n",
      "3214/3214 [==============================] - 87s - loss: 0.9756 - acc: 0.6982 - val_loss: 0.7637 - val_acc: 0.8295\n",
      "Epoch 3/3\n",
      "3214/3214 [==============================] - 87s - loss: 0.9608 - acc: 0.7001 - val_loss: 0.7720 - val_acc: 0.8135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4601d160>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.01\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=3, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 173 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "def image_aug(path, batch_size=4):\n",
    "    return batch_gen(path, zoom_range=0., channel_shift_range=0., horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05, batch_size=batch_size, rotation_range=0.)\n",
    "\n",
    "train_gen = image_aug(samplepath, batch_size=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "173/173 [==============================] - 18s - loss: 0.5996 - acc: 0.8324 - val_loss: 0.7987 - val_acc: 0.7726\n",
      "Epoch 2/6\n",
      "173/173 [==============================] - 15s - loss: 0.5384 - acc: 0.8844 - val_loss: 0.8057 - val_acc: 0.7549\n",
      "Epoch 3/6\n",
      "173/173 [==============================] - 15s - loss: 0.5402 - acc: 0.8671 - val_loss: 0.8270 - val_acc: 0.7549\n",
      "Epoch 4/6\n",
      "173/173 [==============================] - 15s - loss: 0.5673 - acc: 0.8497 - val_loss: 0.8518 - val_acc: 0.7389\n",
      "Epoch 5/6\n",
      "173/173 [==============================] - 15s - loss: 0.4859 - acc: 0.8728 - val_loss: 0.9145 - val_acc: 0.7087\n",
      "Epoch 6/6\n",
      "173/173 [==============================] - 15s - loss: 0.5030 - acc: 0.8902 - val_loss: 0.8805 - val_acc: 0.7194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x46035dd8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.01\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(trainpath, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 94s - loss: 0.8891 - acc: 0.7405 - val_loss: 0.4876 - val_acc: 0.8988\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 84s - loss: 0.6591 - acc: 0.8177 - val_loss: 0.5416 - val_acc: 0.8757\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 85s - loss: 0.5840 - acc: 0.8479 - val_loss: 0.4576 - val_acc: 0.9112\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 84s - loss: 0.5125 - acc: 0.8668 - val_loss: 0.5021 - val_acc: 0.9005\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 82s - loss: 0.4562 - acc: 0.8783 - val_loss: 0.3325 - val_acc: 0.9254\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 82s - loss: 0.4100 - acc: 0.9007 - val_loss: 0.3499 - val_acc: 0.9236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4607b828>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 93s - loss: 0.3978 - acc: 0.8979 - val_loss: 0.3644 - val_acc: 0.9307\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 83s - loss: 0.3642 - acc: 0.9104 - val_loss: 0.3167 - val_acc: 0.9343\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 83s - loss: 0.3288 - acc: 0.9144 - val_loss: 0.3862 - val_acc: 0.9325\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 83s - loss: 0.3199 - acc: 0.9287 - val_loss: 0.3586 - val_acc: 0.9307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4607b8d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 95s - loss: 0.2960 - acc: 0.9334 - val_loss: 0.3151 - val_acc: 0.9343\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 84s - loss: 0.2832 - acc: 0.9331 - val_loss: 0.3483 - val_acc: 0.9343\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 84s - loss: 0.2673 - acc: 0.9403 - val_loss: 0.2646 - val_acc: 0.9449\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 83s - loss: 0.2406 - acc: 0.9452 - val_loss: 0.2609 - val_acc: 0.9609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x460aa860>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 94s - loss: 0.2304 - acc: 0.9477 - val_loss: 0.3240 - val_acc: 0.9361\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 84s - loss: 0.2160 - acc: 0.9536 - val_loss: 0.2720 - val_acc: 0.9520\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 84s - loss: 0.2100 - acc: 0.9561 - val_loss: 0.2792 - val_acc: 0.9485\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 85s - loss: 0.2036 - acc: 0.9527 - val_loss: 0.2753 - val_acc: 0.9414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x460bb828>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3214/3214 [==============================] - 94s - loss: 0.1962 - acc: 0.9564 - val_loss: 0.2355 - val_acc: 0.9627\n",
      "Epoch 2/4\n",
      "3214/3214 [==============================] - 84s - loss: 0.2094 - acc: 0.9546 - val_loss: 0.2369 - val_acc: 0.9520\n",
      "Epoch 3/4\n",
      "3214/3214 [==============================] - 83s - loss: 0.1808 - acc: 0.9605 - val_loss: 0.2405 - val_acc: 0.9627\n",
      "Epoch 4/4\n",
      "3214/3214 [==============================] - 83s - loss: 0.1747 - acc: 0.9655 - val_loss: 0.1840 - val_acc: 0.9698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x460bbc88>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=4, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n",
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(path+'/models/simpler7.h5')\n",
    "generate_submission(path + '/simpler2.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3214 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "def image_aug(path, batch_size=4):\n",
    "    return batch_gen(path, zoom_range=0.1, channel_shift_range=0., horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05, batch_size=batch_size, rotation_range=5)\n",
    "\n",
    "train_gen = image_aug(trainpath, batch_size=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3214/3214 [==============================] - 93s - loss: 0.3975 - acc: 0.8958 - val_loss: 0.2591 - val_acc: 0.9467\n",
      "Epoch 2/6\n",
      "3214/3214 [==============================] - 81s - loss: 0.2913 - acc: 0.9328 - val_loss: 0.2467 - val_acc: 0.9591\n",
      "Epoch 3/6\n",
      "3214/3214 [==============================] - 82s - loss: 0.2507 - acc: 0.9499 - val_loss: 0.2626 - val_acc: 0.9591\n",
      "Epoch 4/6\n",
      "3214/3214 [==============================] - 82s - loss: 0.2488 - acc: 0.9459 - val_loss: 0.2479 - val_acc: 0.9538\n",
      "Epoch 5/6\n",
      "3214/3214 [==============================] - 81s - loss: 0.2195 - acc: 0.9555 - val_loss: 0.2598 - val_acc: 0.9467\n",
      "Epoch 6/6\n",
      "3214/3214 [==============================] - 82s - loss: 0.2204 - acc: 0.9518 - val_loss: 0.2344 - val_acc: 0.9520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x462e8710>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=6, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3214/3214 [==============================] - 95s - loss: 0.2188 - acc: 0.9549 - val_loss: 0.2389 - val_acc: 0.9591\n",
      "Epoch 2/10\n",
      "3214/3214 [==============================] - 85s - loss: 0.2195 - acc: 0.9527 - val_loss: 0.2729 - val_acc: 0.9520\n",
      "Epoch 3/10\n",
      "3214/3214 [==============================] - 86s - loss: 0.2023 - acc: 0.9599 - val_loss: 0.2835 - val_acc: 0.9414\n",
      "Epoch 4/10\n",
      "3214/3214 [==============================] - 86s - loss: 0.2009 - acc: 0.9533 - val_loss: 0.2338 - val_acc: 0.9574\n",
      "Epoch 5/10\n",
      "3214/3214 [==============================] - 83s - loss: 0.1970 - acc: 0.9524 - val_loss: 0.2322 - val_acc: 0.9556\n",
      "Epoch 6/10\n",
      "3214/3214 [==============================] - 83s - loss: 0.1834 - acc: 0.9639 - val_loss: 0.2360 - val_acc: 0.9538\n",
      "Epoch 7/10\n",
      "3214/3214 [==============================] - 83s - loss: 0.1756 - acc: 0.9617 - val_loss: 0.2780 - val_acc: 0.9485\n",
      "Epoch 8/10\n",
      "3214/3214 [==============================] - 82s - loss: 0.1833 - acc: 0.9630 - val_loss: 0.2260 - val_acc: 0.9556\n",
      "Epoch 9/10\n",
      "3214/3214 [==============================] - 82s - loss: 0.1608 - acc: 0.9670 - val_loss: 0.2292 - val_acc: 0.9556\n",
      "Epoch 10/10\n",
      "3214/3214 [==============================] - 82s - loss: 0.1672 - acc: 0.9673 - val_loss: 0.2394 - val_acc: 0.9503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x460ae630>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=10, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3214/3214 [==============================] - 95s - loss: 0.1593 - acc: 0.9736 - val_loss: 0.2380 - val_acc: 0.9556\n",
      "Epoch 2/10\n",
      "3214/3214 [==============================] - 84s - loss: 0.1645 - acc: 0.9676 - val_loss: 0.2242 - val_acc: 0.9627\n",
      "Epoch 3/10\n",
      "3214/3214 [==============================] - 83s - loss: 0.1563 - acc: 0.9695 - val_loss: 0.2586 - val_acc: 0.9538\n",
      "Epoch 4/10\n",
      "3214/3214 [==============================] - 83s - loss: 0.1434 - acc: 0.9729 - val_loss: 0.2144 - val_acc: 0.9574\n",
      "Epoch 5/10\n",
      "3214/3214 [==============================] - 82s - loss: 0.1510 - acc: 0.9704 - val_loss: 0.1999 - val_acc: 0.9591\n",
      "Epoch 6/10\n",
      "3214/3214 [==============================] - 83s - loss: 0.1455 - acc: 0.9714 - val_loss: 0.2443 - val_acc: 0.9520\n",
      "Epoch 7/10\n",
      "3214/3214 [==============================] - 82s - loss: 0.1368 - acc: 0.9754 - val_loss: 0.2008 - val_acc: 0.9574\n",
      "Epoch 8/10\n",
      "3214/3214 [==============================] - 82s - loss: 0.1395 - acc: 0.9729 - val_loss: 0.2121 - val_acc: 0.9591\n",
      "Epoch 9/10\n",
      "3214/3214 [==============================] - 82s - loss: 0.1271 - acc: 0.9782 - val_loss: 0.2073 - val_acc: 0.9591\n",
      "Epoch 10/10\n",
      "3214/3214 [==============================] - 82s - loss: 0.1305 - acc: 0.9764 - val_loss: 0.1761 - val_acc: 0.9680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x460ae710>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=10, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3214/3214 [==============================] - 93s - loss: 0.1282 - acc: 0.9739 - val_loss: 0.2580 - val_acc: 0.9503\n",
      "Epoch 2/10\n",
      "3214/3214 [==============================] - 84s - loss: 0.1290 - acc: 0.9788 - val_loss: 0.1454 - val_acc: 0.9663\n",
      "Epoch 3/10\n",
      "3214/3214 [==============================] - 83s - loss: 0.1324 - acc: 0.9736 - val_loss: 0.2528 - val_acc: 0.9485\n",
      "Epoch 4/10\n",
      "3214/3214 [==============================] - 85s - loss: 0.1204 - acc: 0.9792 - val_loss: 0.1803 - val_acc: 0.9574\n",
      "Epoch 5/10\n",
      "3214/3214 [==============================] - 84s - loss: 0.1166 - acc: 0.9810 - val_loss: 0.2121 - val_acc: 0.9574\n",
      "Epoch 6/10\n",
      "3214/3214 [==============================] - 83s - loss: 0.1193 - acc: 0.9785 - val_loss: 0.2239 - val_acc: 0.9574\n",
      "Epoch 7/10\n",
      "3214/3214 [==============================] - 82s - loss: 0.1197 - acc: 0.9764 - val_loss: 0.2132 - val_acc: 0.9538\n",
      "Epoch 8/10\n",
      "3214/3214 [==============================] - 83s - loss: 0.1152 - acc: 0.9767 - val_loss: 0.1812 - val_acc: 0.9680\n",
      "Epoch 9/10\n",
      "3214/3214 [==============================] - 83s - loss: 0.1190 - acc: 0.9776 - val_loss: 0.2228 - val_acc: 0.9538\n",
      "Epoch 10/10\n",
      "3214/3214 [==============================] - 83s - loss: 0.1112 - acc: 0.9807 - val_loss: 0.1954 - val_acc: 0.9556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x460ae828>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.00001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=10, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/simpler9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n",
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "generate_submission(path + '/simpler3.csv.gz', 0.91)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pump up the resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1080 Ti (CNMeM is disabled, cuDNN 5110)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_1 (BatchNorma (None, 3, 640, 480)   12          batchnormalization_input_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 128, 640, 480) 3584        batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 128, 640, 480) 512         convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 128, 320, 240) 0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 128, 320, 240) 147584      maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 128, 320, 240) 512         convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 160, 120) 0           batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 160, 120) 147584      maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 128, 160, 120) 512         convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 128, 80, 60)   0           batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 614400)        0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           78643328    flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_5 (BatchNorma (None, 128)           512         dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 8)             1032        batchnormalization_5[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 78,945,172\n",
      "Trainable params: 78,944,142\n",
      "Non-trainable params: 1,030\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "path = 'data/fish'\n",
    "trainpath = path + '/train'\n",
    "samplepath = path + '/sample'\n",
    "validpath = path + '/valid'\n",
    "models_path = path+'/models/'\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.core import Lambda, Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "def convblock(model, layers, filters):\n",
    "    for _ in xrange(0, layers):\n",
    "        model.add(Convolution2D(filters, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    #model.add(Dropout(0.1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(axis=1, input_shape=(3, 640, 480)))\n",
    "convblock(model, 1, 128)\n",
    "convblock(model, 1, 128)\n",
    "convblock(model, 1, 128)\n",
    "# convblock(model, 3, 256)\n",
    "# convblock(model, 3, 512)\n",
    "# convblock(model, 3, 512)\n",
    "# convblock(model, 3, 512)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(4096, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def batch_gen(path, class_mode='categorical', shuffle=True, rotation_range=0., zoom_range=0., channel_shift_range=0., horizontal_flip=False, width_shift_range=0., height_shift_range=0., batch_size=4):\n",
    "    return ImageDataGenerator(rotation_range=rotation_range, zoom_range=zoom_range, channel_shift_range=channel_shift_range, horizontal_flip=horizontal_flip, width_shift_range=width_shift_range, height_shift_range=height_shift_range).flow_from_directory(path, target_size=(640, 480), batch_size=batch_size, class_mode=class_mode, shuffle=shuffle)\n",
    "\n",
    "def image_aug(path, batch_size=4):\n",
    "    return batch_gen(path, zoom_range=0., channel_shift_range=0., horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05, batch_size=batch_size, rotation_range=90.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 173 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(samplepath, batch_size=5)\n",
    "valid_gen = batch_gen(validpath, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "173/173 [==============================] - 58s - loss: 2.6688 - acc: 0.2370 - val_loss: 6.2084 - val_acc: 0.1137\n",
      "Epoch 2/5\n",
      "173/173 [==============================] - 58s - loss: 2.0922 - acc: 0.2890 - val_loss: 3.8607 - val_acc: 0.1421\n",
      "Epoch 3/5\n",
      "173/173 [==============================] - 57s - loss: 1.8699 - acc: 0.3757 - val_loss: 2.7940 - val_acc: 0.1261\n",
      "Epoch 4/5\n",
      "173/173 [==============================] - 57s - loss: 1.8440 - acc: 0.3179 - val_loss: 2.7391 - val_acc: 0.1901\n",
      "Epoch 5/5\n",
      "173/173 [==============================] - 57s - loss: 1.7205 - acc: 0.4220 - val_loss: 2.6560 - val_acc: 0.1794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xaf2c63c8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=5, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "173/173 [==============================] - 57s - loss: 2.5903 - acc: 0.2601 - val_loss: 12.8019 - val_acc: 0.1385\n",
      "Epoch 2/5\n",
      "173/173 [==============================] - 57s - loss: 1.9569 - acc: 0.2948 - val_loss: 12.1199 - val_acc: 0.1385\n",
      "Epoch 3/5\n",
      "173/173 [==============================] - 57s - loss: 1.8988 - acc: 0.3410 - val_loss: 10.1240 - val_acc: 0.1297\n",
      "Epoch 4/5\n",
      "173/173 [==============================] - 58s - loss: 1.6642 - acc: 0.4104 - val_loss: 5.6964 - val_acc: 0.1155\n",
      "Epoch 5/5\n",
      "173/173 [==============================] - 57s - loss: 1.8189 - acc: 0.3699 - val_loss: 5.5238 - val_acc: 0.1563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3120ab00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.01\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=5, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "173/173 [==============================] - 58s - loss: 1.5014 - acc: 0.4162 - val_loss: 5.2720 - val_acc: 0.1385\n",
      "Epoch 2/5\n",
      "173/173 [==============================] - 57s - loss: 1.6148 - acc: 0.4046 - val_loss: 4.0422 - val_acc: 0.2078\n",
      "Epoch 3/5\n",
      "173/173 [==============================] - 57s - loss: 1.5852 - acc: 0.4335 - val_loss: 4.1763 - val_acc: 0.1794\n",
      "Epoch 4/5\n",
      "173/173 [==============================] - 58s - loss: 1.5163 - acc: 0.4740 - val_loss: 4.1396 - val_acc: 0.1599\n",
      "Epoch 5/5\n",
      "173/173 [==============================] - 58s - loss: 1.4520 - acc: 0.4277 - val_loss: 4.2580 - val_acc: 0.2256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x75e375c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=5, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "173/173 [==============================] - 58s - loss: 1.6267 - acc: 0.4220 - val_loss: 4.1022 - val_acc: 0.2362\n",
      "Epoch 2/5\n",
      "173/173 [==============================] - 57s - loss: 1.7548 - acc: 0.3468 - val_loss: 6.3816 - val_acc: 0.1510\n",
      "Epoch 3/5\n",
      "173/173 [==============================] - 57s - loss: 1.6794 - acc: 0.4277 - val_loss: 4.7641 - val_acc: 0.1918\n",
      "Epoch 4/5\n",
      "173/173 [==============================] - 57s - loss: 1.5636 - acc: 0.3815 - val_loss: 4.7171 - val_acc: 0.1776\n",
      "Epoch 5/5\n",
      "173/173 [==============================] - 57s - loss: 1.5649 - acc: 0.4220 - val_loss: 4.4663 - val_acc: 0.2060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x75e44b70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=5, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n",
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "generate_submission(path + '/broader1.csv.gz', 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/broader1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "173/173 [==============================] - 59s - loss: 1.7091 - acc: 0.3873 - val_loss: 4.4604 - val_acc: 0.1421\n",
      "Epoch 2/15\n",
      "173/173 [==============================] - 58s - loss: 1.5748 - acc: 0.4335 - val_loss: 4.2296 - val_acc: 0.2025\n",
      "Epoch 3/15\n",
      "173/173 [==============================] - 58s - loss: 1.5433 - acc: 0.4104 - val_loss: 4.1923 - val_acc: 0.2256\n",
      "Epoch 4/15\n",
      "173/173 [==============================] - 58s - loss: 1.4583 - acc: 0.4566 - val_loss: 4.1716 - val_acc: 0.2220\n",
      "Epoch 5/15\n",
      "173/173 [==============================] - 58s - loss: 1.5996 - acc: 0.4509 - val_loss: 4.6377 - val_acc: 0.1599\n",
      "Epoch 6/15\n",
      "173/173 [==============================] - 58s - loss: 1.5989 - acc: 0.4046 - val_loss: 4.1975 - val_acc: 0.2185\n",
      "Epoch 7/15\n",
      "173/173 [==============================] - 58s - loss: 1.4211 - acc: 0.4509 - val_loss: 4.1845 - val_acc: 0.2238\n",
      "Epoch 8/15\n",
      "173/173 [==============================] - 57s - loss: 1.5063 - acc: 0.3988 - val_loss: 4.0142 - val_acc: 0.2345\n",
      "Epoch 9/15\n",
      "173/173 [==============================] - 58s - loss: 1.4757 - acc: 0.4509 - val_loss: 4.0812 - val_acc: 0.2487\n",
      "Epoch 10/15\n",
      "173/173 [==============================] - 58s - loss: 1.4899 - acc: 0.4220 - val_loss: 4.2361 - val_acc: 0.2025\n",
      "Epoch 11/15\n",
      "173/173 [==============================] - 57s - loss: 1.4199 - acc: 0.4451 - val_loss: 4.4534 - val_acc: 0.2202\n",
      "Epoch 12/15\n",
      "173/173 [==============================] - 57s - loss: 1.4704 - acc: 0.4104 - val_loss: 4.4112 - val_acc: 0.2362\n",
      "Epoch 13/15\n",
      "173/173 [==============================] - 58s - loss: 1.4679 - acc: 0.4509 - val_loss: 4.2163 - val_acc: 0.2469\n",
      "Epoch 14/15\n",
      "173/173 [==============================] - 58s - loss: 1.4317 - acc: 0.4624 - val_loss: 4.5623 - val_acc: 0.2060\n",
      "Epoch 15/15\n",
      "173/173 [==============================] - 58s - loss: 1.3536 - acc: 0.4451 - val_loss: 4.4330 - val_acc: 0.2220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x784f5ba8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.00001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=15, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "173/173 [==============================] - 59s - loss: 1.3631 - acc: 0.5087 - val_loss: 4.3654 - val_acc: 0.2558\n",
      "Epoch 2/15\n",
      "173/173 [==============================] - 61s - loss: 1.4254 - acc: 0.4509 - val_loss: 4.3932 - val_acc: 0.2291\n",
      "Epoch 3/15\n",
      "173/173 [==============================] - 59s - loss: 1.3600 - acc: 0.4798 - val_loss: 4.2725 - val_acc: 0.2575\n",
      "Epoch 4/15\n",
      "173/173 [==============================] - 58s - loss: 1.3481 - acc: 0.4855 - val_loss: 4.5690 - val_acc: 0.2540\n",
      "Epoch 5/15\n",
      "173/173 [==============================] - 58s - loss: 1.4350 - acc: 0.4682 - val_loss: 4.3131 - val_acc: 0.2433\n",
      "Epoch 6/15\n",
      "173/173 [==============================] - 59s - loss: 1.3098 - acc: 0.4682 - val_loss: 4.6375 - val_acc: 0.2202\n",
      "Epoch 7/15\n",
      "173/173 [==============================] - 58s - loss: 1.3477 - acc: 0.5318 - val_loss: 4.7081 - val_acc: 0.2522\n",
      "Epoch 8/15\n",
      "173/173 [==============================] - 58s - loss: 1.4562 - acc: 0.4624 - val_loss: 4.5374 - val_acc: 0.2202\n",
      "Epoch 9/15\n",
      "173/173 [==============================] - 57s - loss: 1.3418 - acc: 0.4566 - val_loss: 4.5462 - val_acc: 0.2469\n",
      "Epoch 10/15\n",
      "173/173 [==============================] - 57s - loss: 1.3658 - acc: 0.5029 - val_loss: 4.6288 - val_acc: 0.2362\n",
      "Epoch 11/15\n",
      "173/173 [==============================] - 57s - loss: 1.3800 - acc: 0.4740 - val_loss: 4.2697 - val_acc: 0.2362\n",
      "Epoch 12/15\n",
      "173/173 [==============================] - 57s - loss: 1.3558 - acc: 0.4682 - val_loss: 4.4744 - val_acc: 0.2629\n",
      "Epoch 13/15\n",
      "173/173 [==============================] - 57s - loss: 1.4646 - acc: 0.4393 - val_loss: 4.6979 - val_acc: 0.2948\n",
      "Epoch 14/15\n",
      "173/173 [==============================] - 58s - loss: 1.3427 - acc: 0.4971 - val_loss: 4.7908 - val_acc: 0.2984\n",
      "Epoch 15/15\n",
      "173/173 [==============================] - 58s - loss: 1.3828 - acc: 0.4451 - val_loss: 4.8147 - val_acc: 0.2700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x784e5a58>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=15, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/broader2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_13 (BatchNorm (None, 3, 320, 240)   12          batchnormalization_input_4[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 64, 320, 240)  1792        batchnormalization_13[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_14 (Convolution2D) (None, 64, 320, 240)  36928       convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_14 (BatchNorm (None, 64, 320, 240)  256         convolution2d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_7 (MaxPooling2D)    (None, 64, 160, 120)  0           batchnormalization_14[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_15 (Convolution2D) (None, 128, 160, 120) 73856       maxpooling2d_7[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_16 (Convolution2D) (None, 128, 160, 120) 147584      convolution2d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_15 (BatchNorm (None, 128, 160, 120) 512         convolution2d_16[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_8 (MaxPooling2D)    (None, 128, 80, 60)   0           batchnormalization_15[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 614400)        0           maxpooling2d_8[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 256)           157286656   flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_16 (BatchNorm (None, 256)           1024        dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 8)             2056        batchnormalization_16[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 157,550,676\n",
      "Trainable params: 157,549,774\n",
      "Non-trainable params: 902\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "path = 'data/fish'\n",
    "trainpath = path + '/train'\n",
    "samplepath = path + '/sample'\n",
    "validpath = path + '/valid'\n",
    "models_path = path+'/models/'\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.core import Lambda, Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "def convblock(model, layers, filters):\n",
    "    for _ in xrange(0, layers):\n",
    "        model.add(Convolution2D(filters, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    #model.add(Dropout(0.1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(axis=1, input_shape=(3, 320, 240)))\n",
    "convblock(model, 2, 64)\n",
    "convblock(model, 2, 128)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(4096, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def batch_gen(path, class_mode='categorical', shuffle=True, rotation_range=0., zoom_range=0., channel_shift_range=0., horizontal_flip=False, width_shift_range=0., height_shift_range=0., batch_size=4):\n",
    "    return ImageDataGenerator(rotation_range=rotation_range, zoom_range=zoom_range, channel_shift_range=channel_shift_range, horizontal_flip=horizontal_flip, width_shift_range=width_shift_range, height_shift_range=height_shift_range).flow_from_directory(path, target_size=(320, 240), batch_size=batch_size, class_mode=class_mode, shuffle=shuffle)\n",
    "\n",
    "def image_aug(path, batch_size=4):\n",
    "    return batch_gen(path, zoom_range=0., channel_shift_range=0., horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05, batch_size=batch_size, rotation_range=90.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 173 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_aug(samplepath, batch_size=10)\n",
    "valid_gen = batch_gen(validpath, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "173/173 [==============================] - 22s - loss: 3.2400 - acc: 0.2197 - val_loss: 14.1427 - val_acc: 0.1226\n",
      "Epoch 2/5\n",
      "173/173 [==============================] - 21s - loss: 2.2597 - acc: 0.2948 - val_loss: 14.1713 - val_acc: 0.1208\n",
      "Epoch 3/5\n",
      "173/173 [==============================] - 21s - loss: 2.0067 - acc: 0.3468 - val_loss: 13.8613 - val_acc: 0.1332\n",
      "Epoch 4/5\n",
      "173/173 [==============================] - 21s - loss: 1.9956 - acc: 0.2832 - val_loss: 14.1339 - val_acc: 0.1208\n",
      "Epoch 5/5\n",
      "173/173 [==============================] - 21s - loss: 1.6476 - acc: 0.3931 - val_loss: 11.0235 - val_acc: 0.1208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x46ff3ef0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.01\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=5, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "173/173 [==============================] - 22s - loss: 1.8370 - acc: 0.3526 - val_loss: 12.2675 - val_acc: 0.1226\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 21s - loss: 1.7941 - acc: 0.3121 - val_loss: 11.4167 - val_acc: 0.1261\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 21s - loss: 1.6199 - acc: 0.4393 - val_loss: 9.0673 - val_acc: 0.1190\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 22s - loss: 1.7799 - acc: 0.4104 - val_loss: 7.2737 - val_acc: 0.2007\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 21s - loss: 1.8561 - acc: 0.3410 - val_loss: 5.7777 - val_acc: 0.1048\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 22s - loss: 1.7170 - acc: 0.3526 - val_loss: 5.1735 - val_acc: 0.1634\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 21s - loss: 1.6081 - acc: 0.4220 - val_loss: 6.0533 - val_acc: 0.1243\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 21s - loss: 1.7489 - acc: 0.3873 - val_loss: 5.0481 - val_acc: 0.1741\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 21s - loss: 1.7059 - acc: 0.3584 - val_loss: 4.3976 - val_acc: 0.2043\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 20s - loss: 1.5085 - acc: 0.4451 - val_loss: 4.4767 - val_acc: 0.2060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x46fc87f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.01\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=10, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "173/173 [==============================] - 22s - loss: 1.5337 - acc: 0.4624 - val_loss: 5.0228 - val_acc: 0.1954\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 21s - loss: 1.6869 - acc: 0.4220 - val_loss: 4.3332 - val_acc: 0.2309\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 21s - loss: 1.4602 - acc: 0.4624 - val_loss: 4.4523 - val_acc: 0.2469\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 21s - loss: 1.4697 - acc: 0.4971 - val_loss: 4.5017 - val_acc: 0.2274\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 21s - loss: 1.5564 - acc: 0.4104 - val_loss: 4.3711 - val_acc: 0.2345\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 22s - loss: 1.3781 - acc: 0.4971 - val_loss: 4.4008 - val_acc: 0.2185\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 21s - loss: 1.5176 - acc: 0.4855 - val_loss: 4.2485 - val_acc: 0.2718\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 21s - loss: 1.2613 - acc: 0.5376 - val_loss: 4.5509 - val_acc: 0.2700\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 21s - loss: 1.3683 - acc: 0.4682 - val_loss: 4.4140 - val_acc: 0.2451\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 21s - loss: 1.3947 - acc: 0.5087 - val_loss: 4.6785 - val_acc: 0.2220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x46fc8780>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=10, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 173 images belonging to 8 classes.\n",
      "Found 563 images belonging to 8 classes.\n",
      "Epoch 1/10\n",
      "173/173 [==============================] - 22s - loss: 1.7784 - acc: 0.3815 - val_loss: 7.1835 - val_acc: 0.1155\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 20s - loss: 1.4852 - acc: 0.4451 - val_loss: 6.1015 - val_acc: 0.1226\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 20s - loss: 1.3721 - acc: 0.4798 - val_loss: 5.5007 - val_acc: 0.1297\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 20s - loss: 1.4050 - acc: 0.4798 - val_loss: 4.8847 - val_acc: 0.1439\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 20s - loss: 1.3718 - acc: 0.4509 - val_loss: 5.1518 - val_acc: 0.1243\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 20s - loss: 1.3046 - acc: 0.4971 - val_loss: 4.8243 - val_acc: 0.1705\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 20s - loss: 1.2740 - acc: 0.5549 - val_loss: 4.5763 - val_acc: 0.2060\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 20s - loss: 1.1929 - acc: 0.5145 - val_loss: 4.5472 - val_acc: 0.1865\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 20s - loss: 1.1878 - acc: 0.5665 - val_loss: 4.7143 - val_acc: 0.1936\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 21s - loss: 1.2423 - acc: 0.4913 - val_loss: 4.9388 - val_acc: 0.1918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x44a49fd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen = image_aug(samplepath, batch_size=20)\n",
    "valid_gen = batch_gen(validpath, batch_size=20)\n",
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=10, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "173/173 [==============================] - 22s - loss: 1.2321 - acc: 0.5260 - val_loss: 4.5369 - val_acc: 0.2380\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 20s - loss: 1.1081 - acc: 0.6012 - val_loss: 5.0893 - val_acc: 0.1847\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 20s - loss: 1.2163 - acc: 0.4913 - val_loss: 5.5784 - val_acc: 0.1421\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 20s - loss: 1.1968 - acc: 0.5723 - val_loss: 5.2565 - val_acc: 0.1741\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 20s - loss: 1.2241 - acc: 0.5318 - val_loss: 4.9789 - val_acc: 0.2096\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 20s - loss: 1.2668 - acc: 0.5145 - val_loss: 4.9578 - val_acc: 0.1954\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 21s - loss: 1.1421 - acc: 0.5145 - val_loss: 5.1032 - val_acc: 0.2274\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 20s - loss: 1.1882 - acc: 0.5260 - val_loss: 4.7881 - val_acc: 0.2753\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 20s - loss: 1.0659 - acc: 0.6069 - val_loss: 4.8478 - val_acc: 0.2682\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 20s - loss: 1.0947 - acc: 0.5896 - val_loss: 4.8609 - val_acc: 0.2842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x46fc8da0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=10, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "173/173 [==============================] - 22s - loss: 1.0961 - acc: 0.5838 - val_loss: 4.8412 - val_acc: 0.2522\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 22s - loss: 1.0295 - acc: 0.5896 - val_loss: 5.0344 - val_acc: 0.2664\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 21s - loss: 1.0568 - acc: 0.5838 - val_loss: 5.1990 - val_acc: 0.2842\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 21s - loss: 1.0125 - acc: 0.6069 - val_loss: 4.9887 - val_acc: 0.2806\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 21s - loss: 1.0479 - acc: 0.6185 - val_loss: 5.1566 - val_acc: 0.2753\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 21s - loss: 1.0575 - acc: 0.5549 - val_loss: 5.0430 - val_acc: 0.3020\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 21s - loss: 0.9853 - acc: 0.5954 - val_loss: 4.8895 - val_acc: 0.2931\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 21s - loss: 0.9772 - acc: 0.6301 - val_loss: 4.8925 - val_acc: 0.3197\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 21s - loss: 0.9962 - acc: 0.6474 - val_loss: 4.9822 - val_acc: 0.2771\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 21s - loss: 0.9898 - acc: 0.6012 - val_loss: 4.9741 - val_acc: 0.3197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x46fc8ba8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.01\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=10, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "173/173 [==============================] - 22s - loss: 0.9531 - acc: 0.6763 - val_loss: 4.8612 - val_acc: 0.2824\n",
      "Epoch 2/20\n",
      "173/173 [==============================] - 20s - loss: 0.8461 - acc: 0.7052 - val_loss: 5.0781 - val_acc: 0.3055\n",
      "Epoch 3/20\n",
      "173/173 [==============================] - 20s - loss: 1.0282 - acc: 0.6243 - val_loss: 5.2108 - val_acc: 0.2771\n",
      "Epoch 4/20\n",
      "173/173 [==============================] - 20s - loss: 0.8908 - acc: 0.6358 - val_loss: 5.4787 - val_acc: 0.2718\n",
      "Epoch 5/20\n",
      "173/173 [==============================] - 20s - loss: 0.9595 - acc: 0.6474 - val_loss: 5.0363 - val_acc: 0.3286\n",
      "Epoch 6/20\n",
      "173/173 [==============================] - 20s - loss: 0.9694 - acc: 0.6243 - val_loss: 4.8361 - val_acc: 0.3197\n",
      "Epoch 7/20\n",
      "173/173 [==============================] - 20s - loss: 0.8009 - acc: 0.6705 - val_loss: 5.4453 - val_acc: 0.2895\n",
      "Epoch 8/20\n",
      "173/173 [==============================] - 20s - loss: 0.9292 - acc: 0.5838 - val_loss: 4.9492 - val_acc: 0.2913\n",
      "Epoch 9/20\n",
      "173/173 [==============================] - 20s - loss: 0.8898 - acc: 0.6358 - val_loss: 5.1928 - val_acc: 0.3055\n",
      "Epoch 10/20\n",
      "173/173 [==============================] - 20s - loss: 0.8301 - acc: 0.7052 - val_loss: 5.3293 - val_acc: 0.3215\n",
      "Epoch 11/20\n",
      "173/173 [==============================] - 20s - loss: 0.9937 - acc: 0.6474 - val_loss: 5.1859 - val_acc: 0.3020\n",
      "Epoch 12/20\n",
      "173/173 [==============================] - 20s - loss: 0.9328 - acc: 0.6127 - val_loss: 5.2088 - val_acc: 0.2824\n",
      "Epoch 13/20\n",
      "173/173 [==============================] - 20s - loss: 0.8050 - acc: 0.7225 - val_loss: 5.6473 - val_acc: 0.2860\n",
      "Epoch 14/20\n",
      "173/173 [==============================] - 20s - loss: 0.8440 - acc: 0.6821 - val_loss: 5.4175 - val_acc: 0.3091\n",
      "Epoch 15/20\n",
      "173/173 [==============================] - 20s - loss: 0.8813 - acc: 0.6358 - val_loss: 5.9178 - val_acc: 0.2558\n",
      "Epoch 16/20\n",
      "173/173 [==============================] - 20s - loss: 0.8993 - acc: 0.6705 - val_loss: 6.0485 - val_acc: 0.2718\n",
      "Epoch 17/20\n",
      "173/173 [==============================] - 20s - loss: 0.8910 - acc: 0.6994 - val_loss: 5.6534 - val_acc: 0.3233\n",
      "Epoch 18/20\n",
      "173/173 [==============================] - 20s - loss: 0.7772 - acc: 0.6763 - val_loss: 6.2911 - val_acc: 0.2877\n",
      "Epoch 19/20\n",
      "173/173 [==============================] - 20s - loss: 0.8306 - acc: 0.6590 - val_loss: 6.0937 - val_acc: 0.2913\n",
      "Epoch 20/20\n",
      "173/173 [==============================] - 20s - loss: 0.8236 - acc: 0.6647 - val_loss: 6.2070 - val_acc: 0.2789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x46fc8c50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(train_gen, samples_per_epoch=train_gen.nb_sample, nb_epoch=20, \n",
    "                    validation_data=valid_gen, nb_val_samples=valid_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'/models/sample.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
